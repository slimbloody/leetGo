将 Scalabilty 的需求分成两种：
1. Data Scalability: 单台机器的容量不足以 (经济的) 承载所有资料，所以需要分散。如： NoSQL
2. Computing Scalability: 单台机器的运算能力不足以 (经济的) 及时完成运算，所以需要分散。如：科学运算。


不管是哪一种需求，在决定采用分散式架构时，就几乎注定要接受一些牺牲：
1. 牺牲效率：网路延迟与节点间的协调，都会降低执行效率。
2. 牺牲 AP 弹性：有些在单机上能执行的运算，无法轻易在分散式环境中完成。
3. 牺牲维护维运能力：分散式架构的问题常常很难重现，也很难追踪。
另外，跟单机系统一样，也有一些系统设计上的 tradeoffs
1. CPU 使用效率优化或是 IO 效率优化
2. 读取优化或是写入优化
3. Throughput 优化或是 Latency 优化
4. 资料一致性或是资料可得性


对於资料系统来说，主要的技术手段是 partition 和 replication，再搭配不同的读写方式就
会有很多不同的变化。几个设计决策包括：
• 资料切割
• 读写分工
• 处理颗粒度
• 交易处理
• 资料复制
• 可用性保证
• 错误回复

对於纯运算系统来说，主要的技术手段是资料平行化和运算平行化。如果运算过程中会变
更状态丶或是或参照易变的资料，状况就更加复杂。几个设计决策包括：
• 分工方式
• 讯息交换方式
• 支援的运算种类
• 交易处理
• 状态管理 & Rollback
• 可用性保证


PARTITION
当资料放不进一台机器，或是对资料的运算太过耗时，单台机器无法负荷时，就是考虑 partition 的时候。
partition 就是把资料切割放到多台机器上，首先要考量的，就是要怎麽切资料。

资料有几种常见的切法：
• Round-Robin: 资料轮流进多台机器。好处是 load balance，坏处是不适合有 session或资料相依性 (need join) 的应用。变型是可以用 thread pool，每个机器固定配几个thread，这可以避免某个运算耗时过久，而档到後面运算的问题。
• Range: 事先定好每台机器的防守范围，如 key 在 1~1000 到 A 机器。优点是简单，只需要维护一些 metadata。问题是弹性较差，且会有 hotspot 的问题 (大量资料数值都集中在某个范围)。MongoDB 在早期版本只支援这种切割方式。
• Hash: 用 Hash 来决定资料要在哪台机器上。简单的 Hash 像是取馀数，但取馀数在新增机器时会有资料迁移的问题，所以现在大家比较常用 Consistent Hashing 来避免这个问题。Hash 可以很平均的分布，且只需要非常少的 metadata。但 Hash 规则不好掌握，比方说我们就很难透过自定 Hash 规则让某几笔资料一定要在一起。大部分的 Data Store 都是采用 Consistent Hashing。
• Manual: 手动建一个对照表，优点是想要怎麽分配都可以，缺点是要自己控制资料和负载的均衡，且会有大量 metadata 要维护。


除了切法之外，还要决定用哪个栏位当做切割的 key。

资料切割是非常应用导向的问题，因为有一好没两好，某个切法可能能让某种运算很有效率，但会害到其他种运算。

有一个 tradeoff 在读写之间，优化写可能会害到读。比方说 Round-Robin 可以平均写入负载，但 Range Query 就要到所有机器上查询。而如果用流水号的 Range 来切，有利於流水号的 Range Query，但写入会挤在同一台机器上。
另一个 trade off 在 application 之间，比方说一个用户表格资料如果用地区来切割，那就有利於常带地区条件的应用，因为这些应用可以只锁定几个 partition 进行查询，而不用把query 洒到所有机器上。
为什麽我们不要把 query 洒到所有机器上，这就下次再讲好了。

为什麽有有些时候不要把 QUERY 洒到所有机器上平行处理？
昨天讲到 partition，事实上 partition 比较常用在 write 需求高的应用 (平行写(Parallelism Write))，这是为什麽呢？
以前同事问过一个问题：既然有多台机器，那当然是把 query 分散到多台机器上啊。为什麽我们不想把 query 洒到所有机器上呢？
这问题的答案是：如果 query 很耗时，那分散的确会比较好；但如果 query 很快 (比方有用到 DB 的 index lookup)，那分散会增加效能降低的风险。
Hadoop 的 Map Reduce 就是透过分散提升效率，因为有很多资料要扫，所以分散是值得的。在这种状况下，效能的增加盖过效能降低的风险。
所谓效能降低的风险是指：因为要所有机器都回传资料後才能完成运算，所以运算时间是Max(各台机器的处理时间)。当机器越多的时候，发生异常的机会越高，导致运算延迟。一个现实的例子像是在桥开会时间，当与会人员越多时，就越有可能要花更久时间来协调， 因为要等最慢回覆的那个人。所以，在分散式资料系统中，如过查询费时，可以尽量分散；但如果查询很快，请尽量集中在少数机器处理。


资料切割的 METADATA 管理
今天要谈什麽呢？来谈谈资料切割的 metadata 好了。
现在有好几台机器，都必须要 follow 同一套的资料切割方式，这个切割方式存在 metadata中。这个 metadata 如果不见，那之後的资料就不知道该写入哪一台，且每次查询时都必须要广播找资料，这是很不方便的。所以要想办法保存好这些 metadata。
有些切割方式，像是 Hashing 的 metadata 量非常少，这是相对容易管的。但有些切割方式有很多 metadata，且有些方式在每次 insert 都要更新 metadata (bad practice~)，那这
就麻烦了。
一个最简单的方式就是有一台机器专门管这些 metadata (meta server, config server...)，
若需要 metadata 就来这边问。但明显的这会有单点问题。
现在常见的解法是用 Apache Zookeeper (ZK)，这是一个维持 cluster 中共同状态的分散 式系统，透过 ZK 来维护这些 metadata 是许多分散式系统的普遍作法。ZK 有自己的 HA 和 consistency 机制可以保障资料，而且在 production 环境中一次要用 2n+1 (n>0) 个节 点 (minumum = 3)，只要不要大於 n 个节点挂掉都可以正常服务。因为 ZK 里的资料 很重要!

因为很重要所以至少要保存三份!

ZK 为了保障资料的一致性，存取资料的手续有点麻烦。所以请不要因为 ZK 好用就让 ZK太过操劳。
当然，如果 metadata 真的很少，又不大会更新的时候，连 ZK 都可以省掉。这就是完全 P2P 的系统了，像 Cassandra 就是这种。



REPLICATION
资料复制是维持可用性的方法，因为资料复制好几份到不同机器，所以只要有一台机器还在，资料就拿的到。
但只要有资料复制，就一定会有延迟的状况，也就是在资料复制完成前，多台机器的资料是不一致的。
有的系统对於资料一致性读很要求，就会采同步复制，要复制完成後资料写入才会完成。 但这样会很慢，尤其是副本越来越多的时候。
所以比较有效率的作法是非同步复制，但一定会有一段时间不一致。那有没有折衷作法呢。有的，Quorum 就是一个折衷的作法，R+W>N，你可以控制要write-efficient 还是 read-efficient，然後牺牲另一个 operation 的效率来换资料的一致性。
另外呢，每更新一笔资料就发一次复本更新是很没有效率的，通常要累积一些更新或隔一段时间才会 batch update。
常见的复制是有三个副本，除了原本的资料之外，同一个 rack 或 data center 一个副本， 另一个 rack 或 data center 再一个。
那副本允不允许写入呢？多数资料系统是不允许的，也就是说，副本纯粹只是增加 read concurrency/efficiency/availabilty。同样是副本，同时只有一个 master 副本负责写入，其他的 slave 副本只负责 read request。
也有允许副本写入的系统，像是 cassandra。多个互相冲突的写入会以写入的 timestamp定输赢 (所以 NTP 是必须的，但 NTP 也不能保证多台机器的时间完全一样)，Last write win。当然在还没协调前会存在有资料不一致的时间，那这就是应用必须要忍受的。

