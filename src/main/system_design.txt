核心: 练习要和考试的环节对等


系统设计中常说的Tradeoff是什么
什么叫做SOA (Service Oriented Architecture)
什么是Pull Model &什么是Push Model
数据存储系统有哪些,什么样的数据适合存在什么样的数据存储系统中
什么是异步任务和消息队列(Message Queue)
什么是数据的可持久化(Persistent)
什么是去标准化(Denormalize)
什么是惊群效应(Thundering Herd)
有哪些与NewsFeed类似的系统设计问题?


●设计某某系统Design XXX System
    ●设计微博Design Twitter
    ●设计人人Design Facebook
    ●设计滴滴Design Uber
    ●设计微信Design Whatsapp
    ●设计点评Design Yelp
    ●设计短网址系统Design Tiny URL
    ●设计NoSQL数据库Design NoSQL

●设计某某系统中的某某功能
    ●设计一个功能实现对用户访问频率的限制(输错密码的频率5次限制)
    ●设计一个功能实现统计某个具体事件的历史发生次数(链接在每年被点了多少次)
    ●设计删除一个Tweet的功能
    ●设计邮件系统中将所有邮件标记为已读的功能

系统设计很多时候要做到o(1), 要从数据库的角度上讲怎么实现

设计邮件系统中将所有邮件标记为已读的功能
加一个时间戳记录, xxx时间点过这个按钮
如果时间在xxx之后, 并且读的flag为false, 才是没读
优化掉了写的时间

把上面标记过全部已读的邮件, 从里面再标记为未读
再加一个update_time




1.Work solution
2.Analysis and communication
对设计出什么样的feature达成一致
对存储空间和带宽进行分析
3.Tradeoff Pros/Cons
4.Knowledge Base


步骤
Step 1: Clarify the requirements
Step 2: Capacity Estimation
数据库系统带宽规模估计
Step 3: System APIs
Step 4: High-level System Design
Step 5: Data Storage
Step 6: Scalability

一到两个重点feature考察系统设计的基本功









======================
twitter:
======================
1.Tweet
    a.Create
    b.Delete
2.Timeline/Feed
    а.Home
    b.User
3.Follow a user
4.Like a tweet
5.Search tweets



Consistency
    Every read receives the most recent write or an error
C
    Sacrifice: Eventual consistency(不会很影响用户体验)
Availability
    Every request receives a (non-error) response, without the guarantee that it contains the
most recent write
    Scalable
        Performance: low Latency

Partition tolerance (Fault Tolerance):
1.系统丢包了
2.系统server down了
3.磁盘坏了
The system continues to operate despite an arbitrary number of messages being
dropped (or delayed) by the network between nodes


Step 2: Capacity Estimation
. Assumption:
200 million DAU, 100 million new tweets
Each user: visit home timeline 5 times; other user timeline 3 times
Each timeline/page has 20 tweets
Each tweet has size 280 (140 characters) bytes,
metadata(1.发布时间 2.location) 30 bytes
    Per photo: 200KB, 20% tweets have images
    Per video: 2MB, 10% tweets have video, 30% videos will be watched



Storage Estimate
Write size daily:
1.Text
100M new tweets* (280 + 30) Bytes/tweet = 31GB/day
2.Image
100M new tweets * 20% has image * 200 KB per image = 4TB/day
3.Video
100M new tweets * 10% has video * 2MB per video = 20TB/day

Total
31GB + 4TB + 20TB = 24TB/day



一般都是read heavy的
Bandwidth Estimate
Daily Read Tweets Volume:
200M * (5 home visit +3 user visit) * 20 tweets/page = 32B tweets/day
Daily Read Bandwidth
Text: 32B * 280 bytes 186400 = 100MB/s
Image: 32B * 20% tweets has image * 200 KB per image /86400 = 14GB/s
Video: 32B * 10% tweets has video * 30% got watched * 2MB per video 1 86400 = 20GB/s
Total: 35GB/s

知道哪里是bottle neck, 如何scale这个系统

Step 3: System APls
postTweet(userToken, string tweet)
deleteTweet(userToken, string tweetld)
likeOrUnlikeTweet(userToken, string tweetId, bool like)
readHomeTimeLine(userToken, int pageSize, opt string pageToken)
readUserTimeline(userToken, int pageSize, opt string pageToken)
pageSize: 每次返回多少条
pageToken: 当前读到page的位置, 如果不提供的话会提供最新的tweets

Step 4: High-level System Design
user -> load balancer -> tweet writer -> db
                                |------>写入cache


fan out on write: 扩散写
写扩散也叫：Push、Fan-out或者Write-fanout
读扩散也叫：Pull、Fan-in或者Read-fanout

Home Timeline (cont'd)
Naive solution: Pull mode
How:Fetch tweets from N followers from DB, merge and return
Pros:Write is fast: O(1)
Cons:Read is slow: O(N) DB reads


Home Timeline (cont'd)
Better solution: Push mode
How
1.Maintain a feed list in cache for each user
2.Fanout on write(给每个订阅的用户更新)
Pros
1.Read is fast: O(1) from the feed list in cache
Cons
1.Write needs more efforts: O(N) write for each new tweet:Async tasks
2.Delay in showing latest tweets (eventual consistency)
latency 对用户来说没那么敏感


Fan out on write
Not efficient for users
with huge amount of followers (~>10k)

Hybrid Solution
Non-hot users:
fan out on write (push): write to user timeline cache
do not fanout on non-active users

Hot users:
fan in on read (pull): read during timeline request from tweets cache, and aggregate with
results from non-hot users
大v在读的时候和cache merge在一起不需要更新到用户的timeline里面了


Data Storage
SQL database:E.g: user table
NoSQL database:E.g: timelines
File system:Media file: image, audio, video

Step 6: Scalability
Identify potential bottlenecks
Discussion solutions, focusing on tradeoffs
Data sharding:Data store, cache
data分布在不同的区域, 让traffic read 更scalable

Load balancing:
E.g:
user <-> application server
application server <-> cache server
application server <-> db
大量请求如何assign到server上使每个server更balance
Data caching:Read heavy(大大降低read latency)





sharding: 解决 horizontal scaling的问题
why:Impossible to store/process all data in a single machine
How:Break large tables into smaller shards on multiple servers
Pros:Horizontal scaling
Cons:Complexity (distributed query, resharding)

硬盘和机器增长的速度大于新的tweets产生的速度, 系统就可以无限扩展下去
增加了额外的复杂性

distribute query:
有networking等各种问题
1.比如一个shard比较慢或者宕机了怎么办
2.数据快存满了有resharding的问题

表划分的design option
Option 1: Shard by tweet's creation time:按日期划分
Pros: Limited shards to query
Cons:
1.Hot/Cold data issue
2.New shards fill up quickly


Option 2: Shard by hash(userld): store all the data of a user on a single shard
Pros:
1.Simple
2.Query user timeline is straightforward
Cons:
1.Home timeline still needs to query multiple shards
2.Non-uniform distribution of storage:User data might not be able to fit into a single shard
3.Hot users
4.Availability


Option 3: Shard by hash(tweetld)
Pros:
1.Uniform distribution
2.High availability
Cons:
1.Need to query all shards in order to generate user 1 home timeline


social network: heavy read traffic
read 比 write 高几个数量级

存在cache里面避免hit db
Why:
Social networks have heavy read traffic
(Distributed) queries can be slow and costly

How:
Store hot / precomputed data in memory, reads can be much faster

Timeline service
User timeline: user_id -> {tweet_id}, 缓存个几千条就行了
# Varies alot, T = 1k~100k, Trump: ~60k 缓存个几千条就行了
Home timeline: user_ id -> {tweet_id} # This can be huge, sum(followee' tweets)
Tweets: tweet_ id -> tweet
# Common data that can be shared

feed流放几千条就行了
twitter 只能滚动翻页, 实际上不支持翻页, 前端设计降低后端压力

Topics:
Caching policy: LRU LFU
Sharding: 不可能一个cache server 存下来
Performance: cache 开多少内存
这里最好讲一下你把什么东西存到了cache里，大概估算的用到多少cache的capacity之类的，不然的话可能听者会觉得这么多东西都放到cache里，内存够用么

db: source of truth
-----------------------
●Used push model to fanout news feeds.
●Used Redis to cache all tweets and feeds.
denormalization: 冗余存储
●Used denormalization to store the number of comments & likes in order to minimize db queries.
●Used Redis as Message Queue Broker to deliver asynchronized tasks like email delivery and feeds fanout.


1. user -- give me news feed --> web server
2. web server -- get followings --> friendship database
              <- get followings --

3. web server -- get tweets from followings --> tweet table
              <- get tweets from followings --
4. web server -- merge and return --> user

怎么判断哪个过程是瓶颈? 把用的sql写出来
3的sql: select * from tweets where user in (1, 2, 3...) order by timestamp desc limit 20;
所以是速度最慢的


push 模式:
用户发tweet, 丢给mq, mq get_follower, 然后把信息id丢到 news feed table里面去

=================
Autocomplete
=================
What is system design?
Systems design is the process of defining the architecture, modules, interfaces, and data for a system to
satisfy specified requirements. (wikipedia)

Clarify the requirement
1.SAL (qps 1 latency)
2.Where are the data come from
3.Amount of the data, memory 1 disk 1 cpu 1 machines

Design
1.Architecture / algorithm / data structure 1 DB schema
2.How to scale up? shard VS replica
动态产生replica
3.Backend 1 frontend 1 interface
4.Justify your decision, what are the trade offs?

补全都是热门的补全
每type一个字符都发了一个请求, 返回了前10个最佳的hit
相同的type在前端会保存一个小时
cache-control: private, max-age: 3600
expire: Tue, 19 Feb 2019 01:26:27 GMT


Question: Design a autocomplete system for YouTube
Q: How many entries?
A: 10 Billions.

Q: Where is data coming from?
A: logs.

Q: What are the latency requirements?
A: 100 ms 99 percentile.
server上可能只有30ms

Q: VM spec?
A: 4 CPUS, 16GB RAM, 20GB shared disk.



Using a trie to store the queries.
Each node stores top k frequent of all its children's.
Built offline every week.
Online query: O(L) time

1. (慢)每个节点都返回top10, 然后做merge sort
2. (快)



==================
秒杀系统设计
==================
解决问题
1.瞬时大流量高并发:
服务器、数据库等能承载的QPS有限,如数据库一般是单机1000 QPS。需要根据业务预估并发量。
2.有限库存，不能超卖:
库存是有限的,需要精准地保证,就是卖掉了N个商品。不能超卖,当然也不能少卖了。
3.黄牛恶意请求:
使用脚本模拟用户购买,模拟出十几万个请求去抢购。
4.固定时间开启:
时间到了才能购买,提前- -秒都不可以(以商家「京东」「淘宝」 的时间为准)。
5.严格限购:
一个用户,只能购买1个或N个。


架构:
单体问题:
1. 功能耦合严重
2. 系统复杂, 一个模块升级导致整个服务都升级
3. 扩展性差, 难以对单个模块扩展
4. 开发协作困难, 所有人都在开发同一个仓库
5. cascading failure 级联故障, 一个模块的故障导致整个服务不可用
6. 只能用单一语言
7. 数据库崩溃导致整个服务崩溃(数据库没有分库分表)


micro_service:
1. 秒杀
2. 商品数据库
3. 订单
4. 支付
优势:
1. 各功能模块解耦,保证单一-职责。
2. 系统简单,升级某个服务不影响其他服务。
3. 扩展性强。可对某个服务进行单独扩容或缩容。
4. 各个部门]协作明晰。
5. 故障隔离。某个服务出现故障不完全影响其他服务。
6. 可对不同的服务选用更合适的技术架构或语言。
7. 数据库独立,互不干扰。


商品信息表 commodity_info
商品id 商品名称       商品描述 价格
id    name          desc   price
189   iPhone 11 64G XXXX   5999


秒杀活动表 seckill_info
秒杀id 秒杀名称        商品id        价格    数量
id    name           commodity_id price  number
28|618 iPhone 11 64G|189          |4000| 100

库存信息表 stock_info
库存id  |商品id        |活动id     库存   锁定
id     |commodity_id |seckill_id stock lock
1       189           0          1000000 0
2       189           28         100     5

订单信息表 order_info
订单id 商品id       活动id      用户id    是否付款
id   |commodity_id|seckill_id|user_id| paid
1     189          28         Jack     1

信息流:
商家c端
1. 选择商品信息 commodity_info
2. insert seckill_info
3. insert stock_info

用户侧
1. select seckill_info commodity_info stock_info
2. insert order_info
3. update stock_info


并发导致的超卖: 大家都读到库存是1, 100个人去买, 然后都去减1, 导致超卖

解决方案:
1. 读取 判断的过程加上事务: mysql的并发度很低
1)事务开始:START TRANSACTION;
2)查询库存余量,并锁住数据
SELECT stock FROM 'stock_info'
WHERE commodity_id = 189 AND seckill_id = 28 FOR UPDATE;
for update 是一个行锁
3)扣减库存
UPDATE 'stock_info' SET stock = stock - 1
WHERE commodity_id=189 AND seckill_id= 28;
4)事务提交

2. 使用UPDATE语句自带的行锁
1)查询库存余量
SELECT stock FROM 'stock_info'
WHERE commodity_ jd = 189 AND seckill_id = 28;

2)扣减库存
UPDATE 'stock_info' SET stock = stock - 1
WHERE commodity_id = 189 AND seckill_id = 28 AND stock > 0;
stock > 0 才能操作成功
超卖问题解决了,其他问题呢?
1.大量请求都访问MySQL，导致MySQL崩溃。
对于抢购活动来说,可能几十万人抢100台iPhone ,实际大部分请求都是无效的,不需要下沉到MySQL。

秒杀操作-库存预热
秒杀的本质,就是对库存的抢夺:
每个秒杀的用户来都去数据库查询库存校验库存,然后扣减库存,导致数据库崩溃。

MySQL数据库单点能支撑1000 QPS ,但是Redis单点能支撑10万QPS ,可以考虑将库存信息加载到Redis中:
直接通过Redis来判断并扣减库存。

单线程的数据库。
支持数据的主备容灾(Disaster Tolerance)存储 (有主从)
所有单个指令操作都是原子的，即要么完全执行成功,要么完全执行失败。多个指令也可以通过Lua脚本事务操作
实现原子性。
因为都在内存中操作,性能极高,单机一般可支撑10万数量级的QPS。

什么时候进行预热(Warm-up) ?
活动开始前 双11晚上8点的时候跑定时任务库存预热 把所有库存信息都放到redis里面去

----------
redis扣减库存
redis先读看有没有库存, 再扣减库存
大部分请求都被Redis 挡住了,实际下沉到MySQL的理论上应该就是能创建的订单了。比如只有100台iPhone ,那么到MySQL的请求量理论.上是100。
----------
这个流程有没有问题?
1.检查Redis库存和扣减Redis库存是两步操作。
2.有并发问题仍然会导致超卖。
解决方案
哪怕Redis侧放行,可以创建订单了，到MySQL的时候也需要再检查一次。
----------
新的问题
如果并发量超高, Redis 侧实际超卖的量过大, 如100万个请求同时到达, Redis全部放行。再到MySQL去检测,那Redis作用等于没有。

lua把读库存和写库存放在一起解决这个问题
Lua脚本功能是Reids在2.6版本中推出，通过内嵌对Lua环境的支持Redis解决了长久以来不能高效地处理CAS (check-and-set)命令的缺点，并且可以通过组合使用多个命令轻松实现以前很难实现或者不能高效实现的模式
Lua脚本是类似Redis事务,有一定的原子性,不会被其他命令插队,可以完成一些 Redis事务性的操作

如果秒杀数量是1万台,或者10万台呢?
因为Redis 和MySQL 处理能力的巨大差异。实际下沉到MySQL的量还是巨大, MySQL无法承受
解决思路: 可不可以在通过Redis扣库存后,到MySQL的请求慢一点
解决方案: 通过消息队列(Message Queue , MQ)进行削峰(Peak Clipping)操作


消息队列:
生产者可以高速地向消息队列中投递(生产)消息。
消费者可以按照自己的节奏去消费生产者投递的消息。
消息队列一般带有重试的能力。可以持续投递,直到消费者消费成功。

------------------------
如果消息队列出现部分投递失败怎么办?
Redis中的库存量,可以比实际的库存量多一点,比如1.5倍或者2倍。

创建订单的时候还会对库存进行锁定判断, 就不会导致超卖
------------------------
库存扣减时机

1. 下单时立即减库存。
用户体验最好,控制最精准,只要下单成功,利用数据库锁机制,用户一定能成功付款
可能被恶意下单: 下单后不付款 ,别人也买不了了。

2. 先下单,不减库存。实际支付成功后减库存。
1) 可以有效避免恶意下单
2) 对用户体验极差,因为下单时没有减库存，可能造成用户下单成功但无法付款。

3. 下单后锁定库存,支付成功后,减库存(体验较好)


------------------------
如何限购
------------------------
用户是否有该订单, 无就创建订单
               有就购买失败
要扫所有的订单里面有没有这个商品, 代价很大, 不能用mysql了

把限购商品作为key, 然后把用户的id放到redis set里面


------------------------
付款和减库存的数据一致性
------------------------
付款 -> 支付 -> 订单成功付款 -> 库存扣减
分布式事务: 多个存在于不同数据库的操作, 同时成功或者同时失败

1. 询问是否可以提交 支付 订单 商品
2. 是否可以预执行 支付 订单 商品(先生成一条操作记录 但是不提交)
   执行事务操作, 并将undo(回滚)和redo(重新做)信息记录到事务日志中
3. 提交
   执行事务提交, 或者undo

------------------------
redis扣减库存完毕后, 是否后面请求可以直接拒绝了
------------------------
库存扣减完毕后, 下单按钮置灰

------------------------
防止刷爆商品页面
------------------------
1. cdn eg: 商品图片
2. 点击一次后, 前端下单按钮置灰
3. 部分请求直接跳转到,繁忙页(限流)
4. 未开始抢购时, 禁用抢购按钮
    如何计算倒计时?
    1)打开页面获取活动开始时间, 然后前端页面开始倒计时(如果担心时区不准)
    2)打开页面获取距离活动开始的时间差, 然后前端页面开始倒计时
    3)前端轮询(poll)服务器的时间(校准时钟), 并获取距离活动开始的时间差(时间以服务区为准)

轮询时间会提前一点, 不是完全准确的时间

------------------------
秒杀服务器挂掉,怎么办?
------------------------
尽量不要影响其他服务,尤其是非秒杀商品的正常购买。
服务雪崩(Avalanche)
多个微服务之间调用的时候,假设微服务A调用微服务B和微服务C ,微服务B和微服务C又调用其他的微服务,这就是所谓的”扇出(Fan-out)” , 如扇出的链路上某个微服务的调用响应式过长或者不可用,对微服务A的调用就会占用越来越多的系统资源,进而引起系统雪崩,所谓的”雪崩效应”。
服务雪崩效应是一种因”服务提供者”的不可用导致"服务消费着”的不可用并将这种不可用逐渐放大的过程。


服务熔断(Fuse or Circuit-breaker)
熔断机制是应对雪崩效应的一种微服务链路保护机制,当扇出链路的某个微服务不可用或者响应时间太长时,熔断该节点微服务的调用,快速返回”错误”的响应信息。当检测到该节点微服务响应正常后恢复调用链路。

eg:
Netflix Hystrix
Alibaba Sentinel

------------------------
防止恶意刷请求或者爬虫请求
------------------------
1. 验证码机制(Verification Code Mechanism)
点抢购按钮的时候输入一个验证码

2. 限流机制
限流机制
Ratelimit Mechanism
1. 同一个ip地址, 同一个用户id

------------------------
黑名单机制(风控部门)
------------------------
1. 黑名单ip地址
2. 黑名单用户id

------------------------
秒杀系统 和 订票系统的差异
------------------------
差异主要在 火车票的座位 时段不一样, 每一张票编号都不一样


车次id 车次 车次描述 价格
id  number desc price
189 G1100 xxxx 100


库存信息表: stock_info
库存id 商品id        座位号 库存     锁定
id    commodity_id  seat stock    lock
1       189         1A      1       0
2       189         1B      1       1


----------------------------
Redis在抢购的时候只有扣减, 如果下单失败, 会把扣减的增加回来吗?
1. 一两分钟就抢完了
2. 实际库存会比正常的多一点
----------------------------
如果限购买n次的话, 就把购买次数也记录下来 set改用map
----------------------------
下单后, 锁定库存, 但是不减去库存, 等待一个支付的时间, 用其他的结构存储锁定的库存
----------------------------

todo:
redis 哨兵机制, 集群机制

============
OOD
============
ODD: viability 能不能实现要求的所有功能 eg: design elevator
System Design: Scalability 能不能扩展 eg: design twitter

一亩三分地  面经
glassdoor 面经
--------------




solid:
S: Single responsibility principle
O: Open close principle
L: Liskov substitution principle
I: Interface segregation principle
D: Dependency inversion principle

Single responsibility principle



-------------------------
bad design:
1. print不属于计算类的职责
2. 不好扩展, 如果要加一个xml打印又要在里面加一个函数

public class AreaCalculator
  private float result;
  public float getResult() {
    return this. result;
  }
  public float calculateArea(Triangle t) {
    this.result=h*b/2;
  }
  public void printResultInJson() {
    jsonPrinter.initialize();
    jsonPrinter.print(this. result);
    jsonPrinter.close();
  }
}

-------------------------
public class AreaCalculator {
  private float result;
  public float getResult() {
    return this. result;
  }
  public float calculateArea(Triangle t) {
    this.result = h * b / 2;
  }
}

public class Printer {
  public printInJson( float number) {
    jsonPrinter. initialize();
    jsonPrinter. print(this. result);
    jsonPrinter.close();
  }
}
---------------------------------------------
Open close principle
对象或实体应该对扩展开放，对修改封闭(Open to extension, close to modification)

public class AreaCalculator
  public float calculateArea(Triangle t) {
    //calculates the area for triangle
  }

  public float calculateArea(Rectangle r) {
    //calculates the area for rectangle
  }
}

如果要打不同的图形时要加不同的函数, 很难扩展

解决办法: 用抽象类
public class AreaCalculator {
    private float result;
    public float getResult() {
        return this. result;
    }
    public float calculateArea(Shape s) {
        this.result = s.getArea();
    }
}

----------------
Liskov substitution principle
任何一个子类或派生类应该可以替换它们的基类或父类

I: Interface segregation principle
不应该强迫一个类实现它用不上的接口
----------------
eg:
public class Shape {
  abstract public float calculateVolumn();
  abstract public float calculateArea();
}

平面图形没有体积, calculateVolumn 不应该放进去, 可以搞一个平面图形, 一个立体体积
public class Rectangle extends Shape {
  //...
}

public class Cube extends Shape {
  //...
}


----------------
Dependency inversion principle
抽象不应该依赖于具体实现，具体实现应该依赖于抽象
High-level的实体不应该依赖于low-level的实体
----------------
违背了 open-close 原则:

public class AreaCalculator {
  private float result;
  private Triangle t;

  public float getResult() {
    return this.result;
  }
  public float calculateArea() {
    this. result = t.h * t.b / 2;
  }
}


AreaCalculator 对 Triangle 有依赖
eg:
1. Triangle 被删除了 AreaCalculator不能编译了
2. Triangle的h和b属性改为私有了 AreaCalculator不能编译了


面积计算器 不应该依赖于具体的实现, 应该依赖一个抽象的东西 比如 Shape




=========
OOD 5c
=========
----------------------------
design: glass of water
----------------------------
self ans:
1. 容器: 容量, 形状, 高度
2. 液体的材质, 密度

clarify -> what: 通过和面试官交流，去除题目中的歧义，确定答题范围
            1. 针对关键字通过不断问问题, 帮助自己更好的确定答题范围, 看看需要设计哪种水杯
            2. 大多数的关键字为名词，通过名词的属性来考虑
        -> how: 问哪些问题

Core objects: 确定题目所涉及的类，以及类之间的映射关系

Cases: 确定题目中所需要实现的场景和功能

Classes: 通过类图的方式，具体填充题目中涉及的类

Correctness: 检查自己的设计，是否满足关键点

---------------------------
elevator
---------------------------
clarify:
获取每辆电梯目前的重量
1. What's the weight limit of the elevator?
2. Do we need to consider overweight for our elevator system ?

是否需要设计两种类，如果需要它们之间是什么关系?
客梯和货梯有什么区别?
1. 承重不一样
2. 抵达楼层不一样

所有电梯厢均为相同规格


---------------------------
building
---------------------------
1.楼有多大/楼有多高/楼内能容纳多少人? 通用属性，对于题目帮助不大

2.是否有多处能搭乘的电梯口?
  当收到一个搭乘电梯的请求时，有多少电梯能够响应?

针对本题:每层仅一处能搭乘，所有电梯均可响应


- How
针对问题主题的规则来提问，帮助自己明确解题方向。
此类问题没有标准答案，你可以提出一些解决方法，通过面试官的反应，选择一个你比较有信心(简单)的方案

eg: 如何判断电梯是否超重?
1. Passenger class包含重量
2. 电梯能够自动感应当前重量

eg: 当按下按钮时，哪一台电梯会响应?
1. 同方向>静止>反向
2. 半负责奇数楼层，一半负责偶数楼层

eg: 当电梯在运行时，哪些按键可以响应?
是否能按下反向的楼层

-------------------------------





Core Object
1. 什么是Core Object: 要用到哪些类
2. 为什么要定义Core Object ?
  1) 这是和面试官初步的纸面contract
  2) 承上启下，来自于Clarify的结果，成为Use case的依据
  3) 为画类图打下基础
3. 如何定义Core Object ?
  1) 以一个Object作为基础，线性思考
  2) 确定Objects之间的映射关系

Object:
ElevatorSystem

输入input 输出output

Request -> ElevatorSystem -> Elevator -> ElevatorButton
定义好输入输出
1. 一个请求进来, 会有一个电梯响应这个请求
2. 对应关系:
Request -> ElevatorSystem: 没什么mapping关系
ElevatorSystem -> Elevator: 1对多
Elevator -> ElevatorButton: 1对多

best practice: 写法

Access modifier:
package: 什么都不声明
public: +
private: -
protected: #
如果声明为protected,变量和函数在能被定义他们的类访问的基础上，还能够被该类的子类所访问

在类图中, 避免使用default的package level access
只有写unit test的时候, 才会用这个package level access, 除此之外不用
---------------
cases: use case
---------------
1.什么是Use case ?
在你设计的系统中，需要支持哪些功能?
2.为什么要写Use cases ?
    1) 这是你和面试官白纸黑字达成的第二份共识，把你将要实现的功能列在白板上
    2) 帮助你在解题过程中，理清条例，一个一个Case实现
    3) 作为检查的标准
3.如何写Use cases ?
利用定义的Core Object, 列举出每个Object对应产生的use case.
每个use case只需要先用一句简单的话来描述即可


看看每个类要支持实现哪些场景
(把这些步骤列出来在纸板上 也是为了让面试官一个帮助自己的机会)

ElevatorSystem
1. Handle request

Request
1. N/A

Elevator
1. Take external request
2. Take internal request
3. Open gate
4. Close gate
5. Check weight

ElevatorButton
1. Press button
-----------------------
用类图解答ood

class:
1. 什么是类图?
  类名
  attribute
  function
  通过线段展现之间的关系
2. 为什么要画类图?
  1) 可交付, Minimal Viable Product
  2) 节省时间, 不容易在Coding上挣扎
  3) 建立在Use case上, 和之前的步骤层层递进, 条例清晰, 便于交流和修改
  4) 如果时间允许/面试官要求，便于转化成Code
3. 怎么画类图?
  1) 遍历你所列出的use cases
  2) 对于每-一个use case，更加详细的描述这个use case在做什么事情
  eg: take external request ->
        1. ElevatorSystem takes an external request
        2. and decide to push this request to an appropriate elevator
  3) 针对这个描述，在已有的 core objects 里填充进所需要的信息

把use case当成是这个core object的函数
不需要具体些里面函数的实现



Good Practice
如何知道一一个函数, 是否成功完成任务?
地下一层电梯关闭, 这时有人在地下一层按了向上的按钮, 会发生什么?

1. 设计一个灯, 看灯亮不亮, 多了一个core object, 实现麻烦
2. Use boolean instead of void
   成功的话返回true,否则返回false
   但是不知道里面到底是出了什么问题返回的false
3. 用exception eg: invalidExternalRequestException


怎么设计电梯调度系统
void handleRequest(ExternalRequest r)

1. How do you handle an external request?
2. What if I want to apply different ways to handle external requests during different time of a day?
sol1:
if else
不同时段用不同的handle request
disadvantage: 如果时间一改就很麻烦, 没有open to extension
sol2: strategy design pattern
1. 封装了多种算法/策略, 方便找问题
2. 使得算法/策略之间能够互相替换

一般都是用策略模式组合工厂模式

3. Can you implement it in code?





---------------
其他设计题
parking lot|landing machine
================
interview
================
Good:
1. asked about input
2. asked about null value case



Q: reverse a linked list
Needs improvement
1. 拿到白板题, 先多问点问题, 看看对方想考什么
did not ask enough clarifying question, started coding immediately

2. did not ask about list type
单链表 双链表
3. did not ask output type
4. did not ask about the definition of "reverse"
要清楚定义, 不清楚的当没讲,
eg:
1 2 3 4 5 => 5 4 3 2 1
          => 2 1 4 3 5

5. did not explain his algo, started right away
先解释思路, 解释完再写
6. did not do any null check
   node 本身是null

7. too lttle explanation during the interview process
8. communication needs major improvement
写的过程多和面试官交流, 边写边解释, 参考ted边打war3边解释目的

9. no testing at all
自己写测试用例

10. code not working

------------------
good sol:
------------------
let's assume that all the ambiguity has been clarified
meaning this a singular linked list
there's no circular
and by reverse meaning 1 2 3 4 5 become 5 4 3 2 1
and we are going to return the head of linked list

assume this is our input:   1 -> 2 -> 3 -> null
expected output:            3 -> 2 -> 1 -> null

解释 prev cur next三个节点怎么做

----------------------
核心: 练习要和考试的环节对等

算法/数据结构
CS基础知识
系统设计/OOD
行为类问题
简历准备
面试套路
各大公司面试偏好
Offer谈判能力
英语口语听力
处理模糊信息能力



非技术方面
1.稳定的网络环境
2.合理得体的着装

技术方面
3.提前熟悉工具(Coderpad/Google Doc等)
4.描述能力/阐述能力/解释问题的能力(重中之重)


todo: find an element in a rotated sorted array
the essence of binary search: eliminate half of the unqualified candidates
每一步是如何抛弃掉不合格的一步的



============
hr
============
有部分年轻人的有名气的大中型公司
有年轻人: 交友 话题多
也有老年人: 有人带

1. 避免上一家薪水低的问题, 可以给出其他公司给出的薪水

1. 先定title(先看level是否满意)
2. 整个title的salary range是什么

1. 拿到竞争对手的 competitive offer, negotiation更有力
2. 可以讲一个竞争对手的范围xxx~xxx多了xxx


==================
Uber Project Lead揭秘：在科技巨头如何从0到1，主导项目
==================

分析
1. 研究当前市场机会点是什么
2. 模拟分析, 新的项目上线了能创造多少利润, ROI怎么样
3. 短期 长期 带来的利润和机会

市场潜力
1. Total Addressable Market
2. Serviceable Available Market
3. Serviceable Obtainable Market

技术可行性:
半年或者一年要做得完
    1. 一两个月做完, 前任很可能做完了, 自己做也没太大挑战性
    2. 无人车技术市场空间很大, 大规模商业化可能需要五到十年, 关键在于攻克一些技术难点,知道技术的成熟的速度和空间较为关键

pitch:
最好一句话就能讲完, 能让所有人能理解
1.如果一个方案很大, 背后还有四五个步骤, 验证时间长, 可能很难推进
2.选到小而美的点, 模型较小, 成本低, 易验证

===========================
Lyft Manager袁林：如何高效准备软件工程师（Software Engineer)面试？
===========================
1. 白板面试
2. system design






