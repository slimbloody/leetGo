
公司的现在的体量, 高峰期日订单量上百万高的时候(150w), 日gmv过亿, 详情访问量单节点qps有100多

日访问量有2k多w 


========================================
总体架构
========================================
WAF -> LVS -> nginx -> gateway -> Entry application -> support application -> storage


微服务拆分:
https://microservices.io/
https://microservices.io/patterns/cn/index.html
========================================
大单体拆分
========================================
早期是在大单体上面开发的. 在大单体上面开发, 效率比较低
1. 单体启动速度非常慢, 启动一下要十几分钟, 本地调试和线上故障响应速度都相应被拖慢了, 
2. 团队分工的障碍(协同不方便), 可能一个同事改了代码, 另一个同事在提交的时候又发现有冲突了
3. 扩展能力受限, 无法按需伸缩, 单体应用只能作为一个整体进行扩展, 无法结合业务模块的特点进行伸缩
4. 故障扩散(可靠性差), 可能因为某个同事写出了bug, 单体crash了, 整个服务都不可用了
5. 技术栈受限, 需要长期使用同一个技术栈


拆分为微服务后:
高度可维护和可测试：支持快速和频繁的开发和部署。
与其他微服务松耦合：使团队能够在大部分时间独立地工作在他们自己的微服务上，而不受其他微服务更改导致的影响，同时也不会影响其他微服务。
独立部署：使团队能够部署他们的服务，而不必与其他团队进行协调。
减少沟通成本：可以拆分成小团队专注于各自的微服务，减少大团队内部沟通成本。


根据业务类型, 大致分为了
活动组: 门户, 主管活动的发布浏览, 商品发布浏览
主页组: feed流, 团长和成员的关系, 团长之间的关系
通知组: 通知推送, 订阅关系, 开放平台
订单组: 下单的正向, 逆向流程, 物流, 签到, 短信
运营后台组: 企业微信插件的开发, 运营后台以及一些IM工作
大数据组: 主管大数据业务, 主要生成一些报表信息或者为其他组提供支持工作
架构组: 提供一些基础架构升级, 风控以及审核业务

各组都根据自己负责的模块去拆分对应的接口.

拆分思路:
1. 快速拆出各个组的核心接口, 保证核心流程高可用
2. 在核心流程拆出来以后, 看看大单体还有没有什么访问量比较高的接口
解决这些瓶颈后, 大单体的拆分就不是最紧要的工作了, 以日常开发业务为先, 有空就迁一点出来, 让大单体最后慢慢消亡.

服务自演进: 大服务被拆成小服务后, 如何划清边界成为一个难题. 拆的太细, 增加系统复杂度; 太粗，又达不到预期的效果. 所以整个子服务的边界也应该不断梳理完善,细化,服务需要不断演进.

CAP
BASE
    1. BA：Basically Available，基本可用
        系统出现了不可预知的故障，但还是能用，相比较正常的系统而言会有响应时间上的损失和功能上的损失。
    2. S：Soft State，软状态，状态可以有一段时间不同步
        什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。
        软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。
    3. E：Eventually Consistent，最终一致，最终数据是一致的就可以了，而不是时时保持强一致
        而在实际工程实践中，最终一致性分为5种：
        因果一致性(Causal consistency):如果节点A在更新完某个数据后通知了节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。于此同时，和节点A无因果关系的节点C的数据访问则没有这样的限制。
        读己之所写一致性(Read-your-writes consistency): 节点A更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。
        会话一致性(Session consistency): 会话一致性将对系统数据的访问过程框定在了一个会话当中: 系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值
        单调读一致性(Monotonic read consistency):如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。
        单调写一致性(Monotonic write consistency):一个系统要能够保证来自同一个节点的写操作被顺序的执行。

2pc, 3pc

Nacos:
为什么用nacos做注册中心和配置中心
1. nacos数据结构
2. nacos如何和springcloud整合
3. copyOnWrite
4. 通信改用rpc注册
5. 阈值保护模式: 发现很多节点不健康
   为了防止因过多实例 (Instance) 不健康导致流量全部流向健康实例 (Instance) ，继而造成流量压力把健康 健康实例 (Instance) 压垮并形成雪崩效应，应将健康保护阈值定义为一个 0 到 1 之间的浮点数。当域名健康实例 (Instance) 占总服务实例 (Instance) 的比例小于该值时，无论实例 (Instance) 是否健康，都会将这个实例 (Instance) 返回给客户端。这样做虽然损失了一部分流量，但是保证了集群的剩余健康实例 (Instance) 能正常工作
6. 临时数据, 持久化用raft

监听者设计模式

2. 自动注销实例
4. 多数据中心
5. 跨注册中心同步
1. 雪崩保护
3. 监听支持

1. 灰度

========================================
定时扩容
========================================

观察了一下用户的流量分布, 一般早上10点开始到中午12点和晚上8点到10点都是高峰期, 这个时候数据库的性能还有很多剩余(多个库都是16核128G的), 为了应对高的流量, 一般早上9点开始到中午12点和晚上7点到晚上11点都会定时扩容.

担心有些库日后可能会有数据库性能的问题, 同期也有做一些库表迁移, 把一些iops高的, 但是不紧要的表不停机迁移出去, 如果业务不紧要, 我就直接把一些表砍掉, 比如幂等表.

如何做mysql数据迁移
1. 双写
2. 存量数据迁移
3. 用旧接口双读对比
4. 抽样单读
5. 读新双写
6. canal监控或者阿里云控制台监控
7. rename 数据表, 怕万一还有依赖, 方便把表通过rename再回滚回去
若生产环境因为rename 之后报错, 说明还有接口在读取旧表, 读迁移不完全
8. drop 表

todo:
如何做es数据迁移
1. 索引想加字段
   1. 磁盘空间大, 内存剩余量多, 低峰期reindex(5000w数据大概是40min)
2. 节点性能已经不够了, 要升级节点并且加字段
   1. 直接迁移, 如何压榨写性能

todo:
es索引, es读写数据(官方文档)

如何预估规格: 存量估增量(过去几个月多少, 现在能到多少), 稍微提高一点


========================================
应用被打垮
========================================
之前经常碰到应用被打垮的情况. 这里解释一下什么是应用被打垮. 真正的应用crash要不就是OOM或者 StackOverFlow这种, 应用直接挂掉了. 然而在分布式系统中, 一般都是监控那边发心跳请求给应用, 多次没有响应(我们暂定为4次, 间隔为15s, 5s内一定要给响应), 那么k8s就认为应用已经crash了, 要重启这个节点. 实际上可能出现的事情是, tomcat线程池打满, 同时等待队列也被打满了, 请求被丢弃了, 或者说这种验活请求打到了tomcat的等待队列里面, 但是前面的请求没有处理完, 一直没能响应这个验活请求.

为了解决这个问题, 我们就开始想, 能不能我的验活请求和业务请求分开, 由这个想到说tomcat请求都是从connector这边进来的, 如果我的验活请求或者prometheus监控数据的采集单独分配一个connector, 有自己的独立线程池, 就能解决这个问题. 独立线程池的工作线程数我配少一点嘛, 活跃线程配个5, max配给10足够了.

这个时候还碰到过这种问题, 有个consumer服务是多线程拉取消息处理的, 没有设置阻塞队列大小, 在io密集的业务场景下, 下游服务阻塞, 会导致大量消息堆积到内存中, 最后jvm一直在fullGC, 这种情况下 也没法及时响应liveness probe, 这种属于代码写的有问题的, 所以应用也应该被kill掉重启. 对应小组在应用被kill掉以后会收到钉钉告警, 如果是超过晚上22点会收到告警电话.


========================================
限流
========================================
上了这个以后, 应用被打垮的情况大大减少了. 但是在请求量高的时候, 节点依然腾不出资源给探针响应.
考虑到电商业务场景和腾讯会议那种不一样, 腾讯会议是一小组的用户长时间占用资源, 但电商场景不一样, 单次读完活动信息和商品信息就结束了.
我们是服务提供商, 不可能为这些单一租户提供无限的服务资源的, 所以这个场景做限流是比较合适. 我们这边有类似分销场景, 最高能到三级, 如果超过三级就成传销了. 之前经常有这种情况, 多个活动同时在固定时间点涌进来, 比如晚上8点, 母团子团孙团的用户, 同时在这个时候进来, 看活动详情, 商品详情, 然后下单的, 这个时候我们就对同一母团的活动进行限流. 限流之后就没有这个问题了.


========================================
限流框架选了sentinel
========================================
1. 为什么限流
2. 一般限流会怎么做
3. 为什么选sentinel
   1. java
   2. 阿里
4. sentinel怎么做限流
5. 试用hystrix当时碰到的问题: trace传不进去

========================================
微信自适应限流
========================================
1. 业务怎么通过操作分级(具体业务具体分析)肯定是登陆 > 支付 > 下订单
2. 函数设计, 要让不同机器上面的优先级算出来都一样,每个机器可能自带一张相同的随机数表, 为了避免time drift一定要用nanotime作为seed
3. 最后限流算法的区间是怎么取的

========================================
物理提升服务可用性
========================================
activity-detail这边能不能这样操作, 把节点分成两份, 两份节点都放在不同的ecs上(可用区不同, 防止一台机器挂掉), 给每个节点都限流
缓存:
1. 在这台ecs上面起一个redis节点, 所有的节点共用这一个外部缓存, 同一台机器上能减少网络开销.(自己起的外部redis节点会不会不稳定, 监控怎么办)
2. 根据actId取模, 把请求分布在不同节点上, 但是每个节点自己做自己的缓存(流量分布不均匀, 可能不同节点出现缓存利用率不同的情况), 外部还可以加一个二级缓存.

========================================
单节点顶更多的请求的探索
========================================
1. 虚拟化节点的问题:
首先大部分请求瓶颈都在io上, 关键在于数据库的响应.
最早想的是把4C8G的2节点拆出2C4G的4节点, 这样线程数就翻了一倍
后面想到节点虚拟化需要资源的, 为什么如果要靠这样让总的线程数提升, 为什么不直接调高tomcat的线程数, core线程数和max线程数都可以调大
如果把所有资源都给到一个节点上面, 有SPF问题
总结: 给节点配合理的资源很重要

2. io层面解决问题:
    1. mysql查询优化
        1. 加索引(失败了全部回滚, algorithm=unlock是什么样的)
        2. 拆宽表为长表(减少io的量, 可以把压力给到其他库上面)(适合非常稳定的业务, 不会因为频繁变更业务需求改表)
    2. 异构存储
        1. 查es(实时性不强的)
            1. 倒排索引
            2. 查询优化
        2. hase(es + hbase: tablestore)
            1. 
    3. 加缓存(有些地方是不方便加缓存的)
        1. ds层上层加一个封装, 先写cache层, 通过mq异步把cache刷到mysql里面去
           为什么用ds
            1. 
            2. 
        2. mq失效缓存
        3. 应用外接缓存
        4. 用户回流, 之前的cache早就失效了, 造成慢查询量上升
    
       

    4. 耗时长的请求异步, 非必要的业务流程异步
    5. jvm优化
        1. 对象如何分配的(位置, 空间)
        2. 减少gc的量
            1. 能不能尽量不生成新对象
            2. 对象能不能复用
        3. g1(管理大堆)
        4. cms(在吞吐量方面最优)


========================================
转岗订单订单正向流程
========================================
1. 整个订单正向流程
    1. 订单流程
    2. 下单优化
    3. 商业上的考虑, 不可能用有限的机器提供无限的服务, 加header
    
    1. redis分布式锁(小米)
   
2. 分布式事务
3. es迁移
    1. reindex
    2. 直接迁移
        1. 压榨写性能
        2. 删除操作占满磁盘
        3. 读性能提升
           看了一下测试环境活动es, ghId和actId都是long类型
           用term查询的时候最好改成
           rangeQuery, from:xxx, to: xxx, xxx为同一个值
terms查询就用should包住几个rangeQuery
这样能提升查询速度
4. 大接口迁移
   1. 工期质量和速度的抉择
   2. 双读异步对比

========================================
业务曲线观测
========================================

========================================
技术上共有组件的提供
========================================
 1. 可动态配置的线程池(eventListener 和 配置刷新)
 2. autoconfiguration spi机制
 3. PostConstruct
 4. @Import, 三种方法
 5. ConditionOnProperty
 6. ResponseBodyAdvice,ControllerAdvice
 7. refreshedScoped
 8. 预热 ContextRefreshedEvent
 9. ordered怎么生效的
10. bean生命周期
========================================
系统设计
========================================
1. 短链接
2. mq
3. cache
4. notice
5. rateLimiter
6.

tb:
https://www.bilibili.com/video/BV1DV411B7Jq?spm_id_from=333.999.0.0
https://www.bilibili.com/video/BV18p4y1x72Z?spm_id_from=333.999.0.0
https://www.bilibili.com/video/BV19U4y1h7vZ?spm_id_from=333.999.0.0
https://www.bilibili.com/video/BV1Ah411e79e?spm_id_from=333.999.0.0
https://www.bilibili.com/video/BV1Za411Y7rz?spm_id_from=333.337.search-card.all.click
https://www.bilibili.com/video/BV1534y1S7oU?spm_id_from=333.337.search-card.all.click
========================================
日后优化
========================================
1. 冷热数据分离, 因为这个看到了haystack(long tail)和f4(warm,hot)
2. fb twitter hotring

========================================
业务问题上面的优化
========================================
1. 顺丰物流多个管理员发订单的锁, 之前一直在想怎么锁, 后来一想, 只要把发订单的uid给出去就行了, 让用户自己去处理
2. sys-mina项目引用了一个包画图, 这个包可能有内存泄露, 因为old区每次回收以后, 都比上次old区剩余的对象要多, 而且这个应用的对象大部分都是朝生夕灭的, 排查又没有时间, 就原样复制了一个应用出来, 但是应用名不同, 这个新的应用只处理画图, 到了凌晨4点的时候, 换一个新的画图应用(优雅下线, 会等应用处理完自己内部的请求以后在结束, 把量切到一个新的应用里面去)
3. 第一次分享活动有一个画图的业务, 画图比较慢, 并且是集中在晚上大概七八点的时候, 因为画图没画出来活动是不能分享的, 所以团长会投诉.
   1. 因为分享过的活动是有图的, 即使不重新画图也能分享, 所以分享过的活动再更新图片, 这类请求在高峰期不要优先处理, 这个时候拆分了两个mq, 一个处理画新图, 一个处理画更新图, 把资源集中给新的活动. 
   2. 因为图像迟迟没有画出来, 用户可能会连着画好几次, 服务器重复处理这个请求. 加了一个画图的锁解决这个问题, 在上一个图画完之前锁没有释放, 就不能加锁.
4. 顺丰物流付费, 通知可能被用户静音了, 或者用户没有关注公众号没收到消息.


========================================
在公司看重的点
========================================
1. 新业务产生了效益(比重最大, 让公司有了增长点)(开源)(对接顺丰物流返现)
2. 确确实实用技术降低了公司的成本(节流)(快递100查询要收费, 改用免费的微信物流)
3. 核心业务不出错, 正常使用(bug-free)
4. 人员管理方面, 如何在资源不足的情况下驱动组员(项目动员会, 讲清楚为什么要做这个事情, 更深远的影响)
5. 梯队培养, 快速培养一个核心的人员出来(文档描述清晰, 语言规范)
6. 技术积累, 自己组方案可不可以复用到其他组里面去

========================================
在团队看重的点
========================================
coding 能力
对技术的热情
能简明扼要地沟通
积极乐观
对团队目标的认同

























