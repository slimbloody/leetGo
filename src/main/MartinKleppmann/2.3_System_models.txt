We have seen two thought experiments:
1. Two generals problem: a model of networks
2. Byzantine generals problem: a model of node behaviour

In real systems, both nodes and networks may be faulty!

-----------

Capture assumptions in a system model consisting of:
1. Network behaviour (e.g. message loss)
network are unreliable

2. Node behaviour (e.g. crashes)
3. Timing behaviour (e.g. latency)
Choice of models for each of these parts


======================
Network behaviour/models for network
======================
Assume bidirectional point-to-point communication between two nodes(one sender and one recipient), with one of:

1. Reliable (perfect) links:
A message is received if and only if it is sent. Messages may be reordered.

2. Fair-loss links:
Messages may be lost, duplicated, or reordered. If you keep retrying, a message eventually gets through.

it has a non-zero probability of being delivered.
if you keep repeating the sending of that message then we're going to assume that eventually it will get through. we're not going to make any assumptions about how long that might take

3. Arbitrary links (active adversary):
A malicious adversary may interfere with messages
(eavesdrop, modify, drop, spoof, replay)

if you're connecting to a coffee shop wi-fi, the owner of that wi-fi could be interfering with your network packets in arbitrary ways, and that means thery might not just be looking at your network communication like eavasdropping on it, but they might actually modify the packets, they might record the packets and then replay them at some later point of time, they might pretend to be some website and spoof it, and of course they might drop messages as well.
An arbitrary link is allowed to do any of these things, it's a resonable model of how the internet works today

Network partition: some links dropping/delaying all messages for extended period of time

convert of links:
Arbitrary links -(TLS)-> fair-loss link - retry + deduplicate(重试,去重) -> reliable link
TLS:transport layer security;https

the only thing we cannot do is if the active adversary just decides to block all communication ever in that case nothing is going to get through, and you can't turn that into a fair loss link because we can't make any guarantee that eventually a message will get through if keep retrying


======================
node behavior/models for nodes
======================
Each node executes a specified algorithm, assuming one of the following:

1. Crash-stop (fail-stop):
A node is faulty if it crashes (at any moment).
After crashing, it stops executing forever.

2. Crash-recovery (fail-recovery):
A node may crash at any moment, losing its in-memory state. It may resume executing sometime later.

any in-memory state that it had is not written to disk or to other some non-volatile storage will be lost
data that is stored in stable storage is able to survive the crash and will still be there after the node recovers


3. Byzantine (fail-arbitrary):
A node is faulty if it deviates from the algorithm.
Faulty nodes may do anything, including crashing or malicious behaviour.

in the byzantine generals problem, what it means for a node to be byzantine faulty is it deviates from the algorithm. so we specify an algorithm that all of the nodes are supposed to follow, but a byzantine faulty node may not follow the algorithm, it might pretend to follow the algorithm, it might do stuff to try and make it look honest even though it's actually behaving in some malicious way.
so we're not going to constrain the behavior of byzantine faulty node in any way. we're just going to assume that it can do anything that it wants, including malicious behavior.

A node that is not faulty is called "correct"
we can always categorize a node as either faulty or correct

one node does not necessarily know whether another node is correct or faulty and we will come to the problem of fault detection in a little while

======================
System model: synchrony (timing) assumptions/models for timing
======================
Assume one of the following for network and nodes:

1. Synchronous:
1) Message latency no greater than a known upper bound.
2) Nodes execute algorithm at a known speed.

2. Partically synchronous:
The system is asynchronous for some finite (but unknown) periods of time, synchronous otherwise.

3. Asynchronous: (no timing assumption at all)
Messages can be delayed arbitrarily.
Nodes can pause execution arbitrarily.
No timing guarantees at all.

Note: other parts of computer science use the terms "synchronous" and "asynchronous" differently

we are not making any guarantees about how long it's going to take for messages to arrive, we assume no upper bound on message latency
moreover, we're not going to make any assumptions about how fast nodes are going to execute the algorithm. so we're going to assume that a node might pause its execution at any moment, and just like stop executing its steps for a while and then later resume executing again, and of course this can happen because a thread can be suspended as you know, so a thread can just pause execution for a while and then resume executing sometime later.
so we have here the synchronous model as one extreme like we're making very strong assumptions about the network latency and the node processing speed, adn the asynchronous model where we're making no assumption at all.
so the partially synchronous model is a kind of a compromise between those two where we're saying that actually the asynchronous model is great if we can work in the asynchronous model, but there are certain problems that simply cannot be solved in the asynchronous model. so in some case we do have to make timing assumptions, but at the some time, it's unsafe to assume that those timing assumptions are always true, because if you write an algorithm in the synchronous model, and the system is ever asynchronous, if the system ever take longer than you upper bound to deliver message for example, then your algorithm might fail catastrophically, so the algorithms are very very sensitive to your timing assumption here, and in most cases it is very dangerous to assume a asynchronous model, because real networks do in fact behave in partially synchronous ways, so partially synchronous model is really our compromise where we're saying most of the time the system is well behaved and kind of synchronous, and occasionally it just goes weird, and occasionally messages take a really long time to arrive and occasionally nodes are really slow to execute, and then at some point they'll return back into a synchronous state, but we don't know whether they're asynchronous or synchronous right now.

=============
Violations of synchrony in practice
=============
Networks usually have quite predictable latency, which can occasionally increase:
1. Message loss requiring retry
2. Congestion/contention causing queueing
3. Network/ route reconfiguration

(upgrading a fair loss link to a reliable link, the cost of that upgrade in reliability was that potentially we have to wait for a very long time if there's a network partition, so in this case we can't assume any upper bound really on message latency because it might be up to the length that it takes for our network partition to be healed, other reasons why network latency might suddenly increase is just congestion and queuing in the network, or there have even benn examples of a network reconfiguration where packets just get stuck in a switch buffer for over a minute, before they're eventually delivered. so even in data center networks which are normally very well managed, it is possible occasionally have really extremely high message latency, and so any algorithm that we design must take into account this possibility that occasionally messages might take a very long time to arrive)


Nodes usually execute code at a predictable speed, with occasional pauses:
1. Operating system scheduling issues, e.g. priority inversion
2. Stop-the world garbage collection pauses
3. Page faults, swap, thrashing
Real-time operating systems (RTOS) provide scheduling guarantees. but most distributed systems do not use RTOS

nodes execution might be interrupted for a little while,
you can have a context switch, you can have a thread that temporarily gets suspended while other processes run and these might take a while before the operating system scheduler come back and start running your thread again, especially if there's some kind of problem going on like priority inversion() in the system, it could be a process is actually paused,

priority inversion:
实时操作系统的一个基本要求就是基于优先级的抢占系统。保证优先级高的线程在“第一时间”抢到执行权，是实时系统的第一黄金准则
https://zhuanlan.zhihu.com/p/146132061










thrashing:
操作系统抖动，又叫颠簸，颠簸是不作处理的计算机活动，通常是因为内存或其他资源耗尽或有限而无法完成所要执行的操作。
当操作系统抖动时，程序就会通过操作系统发出请求，操作系统就试图从其他程序中拿来所需的资源，这就使得新的请求不能得到满足。
在虚拟存储系统（使用页来管理逻辑存储或内存的操作系统）中，颠簸就是发生过度页请求操作的情况。

扩展资料
如果分配给进程的存储块数量小于进程所需要的最小值，进程的运行将很频繁地产生缺页中断，这种频率非常高的页面置换现象称为抖动。解决方案优化置换算法。
Belady奇异现象，是指采用页面置换FIFO算法时，如果对一个进程未分配它所要求的全部页面，有时就会出现分配的页面数增多，但缺页率反而提高的异常现象，这是一个违反直觉的现象。

