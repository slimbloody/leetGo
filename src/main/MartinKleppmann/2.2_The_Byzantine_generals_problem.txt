1. we assume the messengers are reliable
2. some of the general are not loyal, being malicious

for points of view of general3, it's impossible to tell the difference between these two scenarios


Byzantine General Problem:
1. Up to f generals might behave maliciously
2. Honest generals don't know who the malicious ones are
3. The malicious generals may collude
4. Nevertheless, honest generals must agree on plan

honest generals don't know who the malicious generals are, but we going to assume some maximum number of generals being malicious
f generals out of n generals in total

honest generals don't know who the malicious generals are,
but malicious generals may know who the other militias generals are, so they might actually work together in some coordinate fashion to try to deceive and trick the honest generals.

and nevertheless our requirement is that the honest generals agree on a plan
so we can't get malicious generals to agree on any part of the plan, because we assuming that they might misbehave in arbitrary ways


============
todo: 证明这个数值
Theorem: need 3f + 1 generals in total to tolerate f malicious generals (i.e. < -1/3 be malicious)
Cryptography (digital signatures) helps - but problem remains hard
============
a digital signature which is a form of message in which it can be proved that a certain party sent a certain message
in this case, it allows general 2 to prove what a command general 1 sent to general 2, and prove that to general 3 in a way that general 3 would be convinced that general 2 really is honest








The Byzantine generals problem [Lamport et al., 1982] has a similar setting to the two generals problem. Again we have armies wanting to capture a city, though in this case there can be three or more. Again generals communicate by messengers, although this time we assume that if a message is sent, it is always delivered correctly



The challenge in the Byzantine setting is that some generals might be “traitors”: that is, they might try to deliberately and maliciously mislead and confuse the other generals. One example of such malicious behaviour is shown on Slide 27: here, general 3 receives two contradictory messages from generals 1 and 2. General 1 tells general 3 to attack, whereas general 2 claims that general 1 ordered a retreat. It is impossible for general 3 to determine whether general 2 is lying (the first case), or whether general 2 is honest while general 1 is issuing contradictory orders (the second case).


The honest generals don’t know who the malicious generals are, but the malicious generals may collude and secretly coordinate their actions. We might even assume that all of the malicious generals are controlled by an evil adversary. The Byzantine generals problem is then to ensure that all honest generals agree on the same plan (e.g. whether to attack or to retreat). It is impossible to specify what the malicious generals are going to do, so the best we can manage is to get the honest generals to agree.



This is difficult: in fact, it can be proved that some variants of the problem can be solved only if strictly fewer than one third of the generals are malicious. That is, in a system with 3f + 1 generals, no more than f may be malicious. For example, a system with 4 generals can tolerate f = 1 malicious general, and a system with 7 generals can tolerate f = 2.


The problem is made somewhat easier if generals use cryptography (digital signatures) to prove who said what: for example, this would allow general 2 to prove to general 3 what general 1’s order was, and thus demonstrate general 2’s honesty. We will not go into details of digital signatures in this course, as they are covered in the Security course (Part IB Easter term). However, even with signatures, the Byzantine generals problem remains challenging.

Is the Byzantine generals problem of practical relevance? Real distributed systems do often involve complex trust relationships. For example, a customer needs to trust an online shop to actually deliver the goods they ordered, although they can dispute the payment via their bank if the goods never arrive or if they get charged too much. But if an online shop somehow allowed customers to order goods without paying for them, this weakness would no doubt be exploited by fraudsters, so the shop must assume that customers are potentially malicious. On the other hand, for RPC between services belonging to the shop, running in the same datacenter, one service can probably trust the other services run by the same company. The payments service doesn’t fully trust the shop, since someone might set up a fraudulent shop or use stolen credit card numbers, but the shop probably does trust the payments service. And so on. And in the end, we want the customer, the online shop, and the payments service to agree on any order that is placed. The Byzantine generals problem is a simplification of such complex trust relationships, but it is a good starting point for studying systems in which some participants might behave maliciously.





















