============================================
Eventual consistency
============================================
Linearizability advantages:
    Makes a distributed system behave as if it were non-distributed
    Simple for applications to use

Downsides:
    Performance cost: lots of messages and waiting for responses
    Scalability limits: leader can be a bottleneck
    Availability problems: if you can't contact a quorum of nodes, you can t process any operations

Eventual consistency: a weaker model than linearizability.
Different trade-off choices.
-----------------------------------------------------------
so we've talked about linearizability as one possible consistency model for distributed systems, which is a very strong consistency model, and we said it has the very appealing property that is actually makes a replicated system look as if it were not replicated, it looks as if there was only a single copy of the data and all of the operations on it are atomic, which makes linearizablity a very easy to use consistency model. you basically don't have to think about the fact that your system is distributed, and it just kind of works in the way you might expect.
unfortunately, it comes at a price as well, so linearizability is not actually suitable in all possible circumstances for several reasons:
one possible reason is that it's actually quite expensive to implement.
if you look at the protocols that we discussed there are lots of messages going back and forth, lots of waiting for responses probably something like around two-round trips for most requests. so you know it's just going to be a bit slow basically.

another limitation is the scalability potentially depends a bit on the exact algorithm you're using but if you're using a consensus algorithm like raft for example, all of the updates have to be sequenced through the leader, so there's a single node that all of the updates have to flow through, and so you are limited by the capacity of that one node in this kind of system that one leader node can become a bottleneck.

and what is perhaps the deepest problem with linearizability, and such strong consistency models is problems with availability, and that is every operation that you do needs to contact a quorum of nodes, and that means if you can't contact a quorum of nodes for whatever reason, maybe you're disconnected on the network, or so you can't process any operations, neither reading or writing is possible, if you can't contact a quorum of nodes.
and that's why alternative consistency models have been developed which have different trade-offs, which are better in some regards and worse in other regards. and the one that we will look at in this section is called eventually consistency
============================================
Calendar app example
============================================
as an example, let's consider a calendar app.
so here I have on the left hand side, the calendar app running on my computer, and on the right hand side the calendar app running on my phone. and you all know how this works, I can add an event on the left hand side say "distributed systems lecture", and this will sync within a couple of seconds over to the phone on the right hand side. so then, the same event should appear there, here we go.
and similarly, I can add an event on the phone, and this should sync over to the computer.
now what we've got here if you think about it is a replicated system, we've got a copy of the calendar on my computer, and another copy of calendar on my phone, and these two replicas are independent from each other in the sense that I can put my phone into airplane mode, and now I have prevented any communication from the between these two devices from happening. and so I could, for example, on my computer now change the time of the distributed systems lecture to 10.a.m, and this won't sync over to my phone because my phone is unable to communicate, and let's say on my phone I change the title, to say "distributed system lecture 1". and this also won't sync back to the computer until I eventually turn off airplane mode. so now after airplane mode is off now, the two device should be able to sync up with each other again, and the change from one should propagate over to the other.
so you can see what has happened now is the time of this event has moved to 10.a.m on both devices, but my change, changing the title "distributed systems lecture", i changed it to "lecture 1", but that change of the title has somehow disappeared, and so what we have here is a conflict, because I concurrently edited the same calendar event on two different devices, while they were disconnected from each other, and we had a conflict resolution that had to happen here, and what my calendar app did apparently was a last writer wins resolution, in this case the update of the time to 10 a.m, somehow took precedence over the update of the title, and the update of the title was discarded while the update of the time took effect on both devices, but even though some data was last here, but at least we did end up with both devices being in a consistent state eventually.



============================================
The CAP theorem
============================================
A system can be either strongly Consistent (linearizable) or Available in the presence of a network Partition these systems such like the calendar app can be formalized.
----------------------------------------------------
these systems such like the calendar app can be formalized using this result from distributed system called the CAP theorem.
and this idea behind this is fairly simple. it is say you have a system in which there's a network partition that is some of the nodes, one subset of the nodes is unable to communicate with some other subset of the nodes.
in this case, we have to make a choice:
either we can continue providing linearizability, but some of the nodes won't be able to process requests,
or we allow operations to continue without contacting a quorum, but in that case, we risk violating linearizability.

and so we have to just make this choice in the presence of a network partition.

illustrated here so we've got on the left hand side of the partition nodes a and b, which form a quorum, and they can continue providing a linearizable service for these operations.
but node c here on the right hand side is stuck in this dilemma, because either the node c has to wait for this network partition to be healed before it can reach a quorum, which might take indefinite amount of time, and so in that during that time node c is effectively unavailable and not able to process any requests, or node c just goes ahead and uses its local state but in that case it won't know about the value v1 that was written by a. so in this case, we will have violated linearizability.
and so this is just a fundamental choice that you have to make in a system which network partitions might occur.


============================================
Eventual consistency
============================================

Replicas process operations based only on their local state.

If there are no more updates, eventually all replicas will be in the same state. (No guarantees how long it might take. )


--------------------------------------------
and eventual consistency is sort of the broad category of consistency models that typically people move to, if they don't want to make an assumption of being able to communicate with a quorum something like that.
so if we want each replica to be able to process operations just based on its local state and without waiting for a communication with a quorum, we cannot achieve linearizability, but we can achieve weaker consistency models.
and the way eventual consistency is usually defined is like this, first of all, any read requests, a replica can just process based on its local state, and then whenever some updates get applied to one replica, they will eventually wander over the network at some point to other replicas, and what we want to guarantee is that if we assume that ###at some point the updates stop###, then eventually after the updates have stopped, then all of the replicas end up in the same state.
now this is okay, but it's a fairly weak property, because for example what if the updates never stop, you could have a system in which continually updates our process non-stop, and so in this case eventual consistency does not define what will happen in that case, because the premise of this statement of this definition is not true, so what people have done is to define an alternative slight strengthening of eventual consistency called strong eventual consistency which is defined with two properties.


--------------------------------------------
Strong eventual consistency:
    1. Eventual delivery: every update made to one non-faulty replica is eventually processed by every non-faulty replica.
    2. Convergence: any two replicas that have processed the same set of updates are in the same state
    (even if updates were processed in a different order).

--------------------------------------------
so first of all, we say that any update that has been made on one replica will eventually also be made on another replica, so updates get disseminated over the network, we don't make any guarantees about how long that's going to take, we just say eventually this will happen.
and secondly, whenever you have two replicas that have processed the same set of updates, then those replicas must be in the same state and thus they will return the same results to any read requests. and this must hold even if those updates were actually applied in a different order on different replicas, and so this comes back down to the causal broadcast based replication protocols that we saw earlier for example, where if you have commutative operations for operating the state, then you can have different replicas updating the state in different order and still converging to the same final state, and so this idea of convergence is really at the core of eventual consistency and strong eventual consistency.



--------------------------------------------

Properties:
    1. Does not require waiting for network communication
    2. Causal broadcast (or weaker) can disseminate updates

C must either wait indefinitely for the network to recover, or return a potentially stale value.
--------------------------------------------

now both eventual consistency and strong eventual consistency have some very nice properties, if you want to implement this model, there is no need ever for an operation to wait for communication with another replica, so a replica can always process operations both reads and writes can be processed just locally without waiting for any communication.
of course eventually sometime communication will have to happen in the background, but the operations don't have to wait for it, which means that operations can always be fast, they're never waiting for network round trips, and the operations can always be reliable, because even if the network is interrupted, and a replica can continue processing operations, and we can use these broadcast protocols of causal broadcast, of FIFO broadcast or these other weaker broadcasts as a way of distributing the updates from one replica to another.
now one problem that can arise with eventually consistent systems is that the same object or the same data item can be concurrently updated by multiple replicas and there's nothing stopping this of course because they cannot communicate necessarily and so because we have these concurrent updates of the same object, we might have conflicts, and so those conflicts will have to be resolved in some way or another.
with the calendar app, what we saw is just the last writer wins policy where one of the updates took precedence over the other, the other one was just discarded, but we can also devise more sophisticated conflict resolution policies, and I've actually done some research in this area on merge algorithms, which will take concurrently updated data structures and merge them together into a clean final state to resolve conflict.

--------------------------------------------



so that's eventual consistency, I'll now recap the consistency models and algorithms that we talked about in this lecture, and do that in the order of how strong the assumptions are that they make about the system model, this is an interesting way of comparing these different algorithms and approaches.
============================================
Summary of minimum system model requirements
============================================



                                                       strength of assumptions
----------------------------------------------------------------------------^
Problem               | Must wait for communication | Requires synchrony    |
----------------------------------------------------------------------------|
atomic commit         | all participating nodes     | partially synchronous |
----------------------------------------------------------------------------|
consensus,            | quorum                      | partially             |
total order broadcast,|                             | synchronous           ^
linearizable CAS      |                             |                       |
----------------------------------------------------------------------------|
linearizable get/set  | quorum                      | asynchronous          |
----------------------------------------------------------------------------|
eventual consistency, | local replica only          | asynchronous          |
causal broadcast,     |                             |                       ^
FIFO broadcast        |                             |                       |
----------------------------------------------------------------------------|

----------------------------------------------------------------------------
so first of all we started with atomic commitment as in two-phase commit.
and remember that the model with atomic commit is that a transaction can commit only if all of the participating nodes vote to commit.
and so this means that inevitably atomic commit requires communicating with all participating nodes, which might be all nodes in the system, so if any one node is crashed, atomic commit will not be able to commit any transactions, so this is very strong assumption that we have to make about the communication that happens in the system, we have to communicate with all nodes and wait for response.
----------------------------------------------------------------------------

----------------------------------------------------------------------------
moreover, we have to assume a partially synchronous system model, because we have timeouts and failure detection that has to happen.
### todo: FLP result
now, one step down from this is consensus and total order broadcast, and so consensus and total order broadcast, the FLP result told us that this also requires a partially synchronous model, so the FLP result says that there's no asynchronous algorithm for implementing consensus, and so therefore partial synchrony is what we have to assume, but in terms of the communication, these consensus actually assumes a bit less, because it only requires communication with a quorum, and so therefore, we can tolerate a minority of nodes being unavailable, and the algorithm will still be able to make progress and complete as normal, and so this holds for consensus and total order broadcast which are equivalent to each other as we said.
moreover, we saw that for linearizable compare and swap, we can implement that using total order broadcast, and in fact it is possible to prove that linearizable compare and swap is also equivalent to consensus. so all these three problems are actually reducible to each other, they all have the property that they require a quorum and partial synchrony.
----------------------------------------------------------------------------

----------------------------------------------------------------------------
moving down in the list, interestingly a linearizable get and set operation has weaker assumptions than a linearizable compare and swap operation.
because for get and set, we can just use the approach we saw earlier the ABD algorithm which means quorum writes and quorum reads with read repair. and this still requires communicating with a quorum of nodes, but notice that in this algorithm, there are no timers, there are no timeouts, there's no failure detection, the algorithm is actually completely asynchronous, it doesn't require on clocks at all. and so this means that actually linearizable get and set, has weaker assumptions about the system model than compare and swap.
----------------------------------------------------------------------------

----------------------------------------------------------------------------
and finally as we move all the way down to the weakest assumptions that we can make, now we're with eventual consistency, where every replica can update and read its own local state without waiting for any communication with any other replica.
so the communication need is totally minimized here, and we still making no timing assumptions, so this is about the weakest that we can assume for both eventual consistency and strong eventual consistency.
and also in terms of the broadcast protocols, causal broadcast, FIFO broadcast, reliable broadcast, and best effort broadcast, all of these have the property that they can deliver a message to themselves immediately, a node doesn't have to wait for communication with any other node in order to deliver a message to itself, and this thus also has the property that if these messages are updates, that means updates can be applied locally without waiting for any communication, so this also has the property that no waiting for communication is required in order to process local updates.
----------------------------------------------------------------------------


so that's all on consistency models for today! (^_^)













