===============
problem statement
===============
流量突增会打垮应用.

流量突增的原因
1. our client is another popular web service and it experienced a sudden traffic spike.
2. Or developers of that web service started to run a load test.
3. Or this is just a malicious client who tried to DDoS our service.

All these situations may lead to a so called
"noisy neighbor problem", when one client utilizes too much shared resources on a service
host(服务宿主), like CPU, memory, disk or network I/O.
And because of this, other clients of our
application start to experience higher latency for their requests, or higher rate of failed
requests.

====================
处理办法:
====================
One of the ways to solve a "noisy neighbor
problem" is to introduce a rate limiting (also known as throttling).

Throttling helps to limit the number of requests
a client can submit in a given amount of time.

Requests submitted over the limit are either immediately rejected or their processing is delayed.



======================
question 1
======================
(traffic spike) It should be solved by scaling out the cluster of hosts that run our web service. And ideally, by some kind of auto-scaling, right?

======================
ans 1
======================
And the problem with scaling up or scaling out is that it is not happening immediately. Even autoscaling takes time. And by the time scaling process completes it may already be late. Our service may already crash.

1. 来不来得及
2. 扩容不一定能解决问题, 可能让问题更糟

其实不止是应用的crash, 首先看流量起来了压力在哪里, 像数据库这种不能无限扩容的, 如果应用本身就很多, 硬起应用很可能就把数据库直接打垮了.


======================
question 2
======================
And the next question I hear from you is about other means of how rate limiting can be achieved.

Specifically, you mention load balancers and their ability to limit a number of simultaneous requests that load balancer sends to each application server.

Load balancers indeed may prevent too many requests to be forwarded to an application server. Load balancer will either reject any request over the limit or send the request to a queue, so that it can be processed later.

But the problem with this mechanism - it is indiscriminate.

Let’s say our web service exposes several different operations. Some of them are fast operations, they take little time to complete.

But some operations are slow and heavy and each request may take a lot of processing power.
Load balancer does not have knowledge about a cost of each operation.

And if we want to limit number of requests for a particular operation, we can do this on application server only, not at a load balancer level.




===============================
But one question is still sitting in your head.

The problem does not seem to be a system design problem.
===============================
Algorithmic problem?
Yes, as we need to define data structures and algorithm to count how many requests client has made so far.

Object-oriented design problem?
Probably, as we may need to design a set of classes to manage throttling rules. Rules define an allowed throttling limit for each operation.


So, if we implement throttling for a single host, are we done?

In an ideal world - yes.
But not in the real world.

Your thought process is very reasonable.

If we have a load balancer in front of our
web service, and this load balancer spreads requests evenly across application servers, and each request takes the same amount of time to complete - you are right.
In this case this is a single instance problem and there is no need in any distributed solution.

Application servers do not need to talk to each other. They throttle requests independently.


But in the real-world load balancers cannot distribute requests in a perfectly even manner.

Plus, as we discussed before different web service operations cost differently.

And each application server itself may become slow due to software failures or overheated,
due to some other background process running on it.

All this leads to a conclusion that we will need a solution where application servers will communicate with each other and share information about how many client requests each one of them processed so far.

If after this conversation between me and myself on your behalf you do not consider me crazy, let's move on and formalize requirements.

Both functional and non-functional.

Functional requirements are simple.

For a given request our rate limiting solution should return a boolean value, whether request is throttled or not.

As for non-functional requirements we need rate limiter to be fast (as it will be called on every request to the service), accurate (as we do not want to throttle customers unless it is absolutely required), and scalable (so that rate limiter scales out together with the service itself).

94
00:06:02,720 --> 00:06:07,160
If we need to add more hosts to the web service
cluster, this should not be a problem for

95
00:06:07,160 --> 00:06:09,330
the rate limiter.

96
00:06:09,330 --> 00:06:12,050
What other requirements you can think of?

97
00:06:12,050 --> 00:06:15,420
What else the interviewer may be interested
in?

98
00:06:15,420 --> 00:06:18,490
What about high availability and fault tolerance?

99
00:06:18,490 --> 00:06:21,180
Two common requirements for many distributed
systems.

100
00:06:21,180 --> 00:06:24,690
Are they important for a rate limiting solution?

101
00:06:24,690 --> 00:06:26,590
Mmm...not so much.

102
00:06:26,590 --> 00:06:32,710
If rate limiter cannot make a decision quickly
due to any failures, the decision is always

103
00:06:32,710 --> 00:06:34,270
not to throttle.

104
00:06:34,270 --> 00:06:36,360
And this makes a lot of sense, right?

105
00:06:36,360 --> 00:06:40,500
If we do not know whether throttle or not
- we do not throttle.

106
00:06:40,500 --> 00:06:45,270
Because we may need to introduce rate limiting
in many services, the interviewer may ask

107
00:06:45,270 --> 00:06:48,740
us to think about ease of integration.

108
00:06:48,740 --> 00:06:54,000
So that every service team in the organization
can integrate with our rate limiting solution

109
00:06:54,000 --> 00:06:55,420
as seamlessly as possible.

110
00:06:55,420 --> 00:06:57,500
This is a good requirement.

111
00:06:57,500 --> 00:07:00,540
And we will talk more about it later in this
video.

112
00:07:00,540 --> 00:07:04,050
Ok, so we are done with the requirements.

113
00:07:04,050 --> 00:07:07,090
And ready to start building a solution.

114
00:07:07,090 --> 00:07:13,230
And here I want to start with the recommendation
that you’ve heard millions of times already.

115
00:07:13,230 --> 00:07:15,460
Start with a simple solution first.

116
00:07:15,460 --> 00:07:21,530
This recommendation is everywhere out there:
books, blogs, training videos.

117
00:07:21,530 --> 00:07:26,030
And it is amazing how many candidates ignore
it, unfortunately.

118
00:07:26,030 --> 00:07:32,080
Whether this is an algorithmic problem or
system design problem, it always makes sense

119
00:07:32,080 --> 00:07:37,230
to start with (or at least mention to the
interviewer) a simple solution and evolve

120
00:07:37,230 --> 00:07:40,270
the solution along the interview.

121
00:07:40,270 --> 00:07:45,680
With this in mind, let’s implement a rate
limiting solution for a single server first.

122
00:07:45,680 --> 00:07:49,580
So, no communication between servers just
yet.

123
00:07:49,580 --> 00:07:55,400
The first citizen of the rate limiting solution
on the service host is the rules retriever.

124
00:07:55,400 --> 00:08:00,610
Each rule specifies a number of requests allowed
for a particular client per second.

125
00:08:00,610 --> 00:08:06,139
These rules are defined by service owners
and stored in a database.

126
00:08:06,139 --> 00:08:10,610
And there is a web service that manages all
the operation with rules.

127
00:08:10,610 --> 00:08:15,580
Rules retriever is a background process that
polls Rules service periodically to check

128
00:08:15,580 --> 00:08:18,919
if there are any new or modified rules.

129
00:08:18,919 --> 00:08:23,230
Rules retriever stores rules in memory on
the host.

130
00:08:23,230 --> 00:08:28,310
When request comes, the first thing we need
to do is to build a client identifier.

131
00:08:28,310 --> 00:08:32,130
Let’s call it a key, for short.

132
00:08:32,130 --> 00:08:37,099
This may be a login for registered clients
or remote IP address or some combination of

133
00:08:37,099 --> 00:08:40,589
attributes that uniquely identify the client.

134
00:08:40,589 --> 00:08:44,639
The key is then passed to the Rate Limiter
component, that is responsible for making

135
00:08:44,639 --> 00:08:46,509
a decision.

136
00:08:46,509 --> 00:08:50,200
Rate Limiter checks the key against rules
in the cache.

137
00:08:50,200 --> 00:08:55,810
And if match is found, Rate Limiter checks
if number of requests made by the client for

138
00:08:55,810 --> 00:08:59,800
the last second is below a limit specified
in the rule.

139
00:08:59,800 --> 00:09:05,040
If threshold is not exceeded, request is passed
further for processing.

140
00:09:05,040 --> 00:09:08,569
If threshold is exceeded, the request is rejected.

141
00:09:08,569 --> 00:09:12,740
And there are three possible options in this
case.

142
00:09:12,740 --> 00:09:17,660
Our service may return a specific response
status code, for example service unavailable

143
00:09:17,660 --> 00:09:19,769
or too many requests.

144
00:09:19,769 --> 00:09:22,189
Or we can queue this request and process it
later.

145
00:09:22,189 --> 00:09:26,370
Or we can simply drop this request on the
floor.

146
00:09:26,370 --> 00:09:28,309
Nothing scary so far, right?

147
00:09:28,309 --> 00:09:31,870
The thinking process has been pretty straightforward.

148
00:09:31,870 --> 00:09:35,420
We know we need a database to store the rules.

149
00:09:35,420 --> 00:09:40,269
And we need a service on top of this database
for all the so-called CRUD operations (create,

150
00:09:40,269 --> 00:09:42,350
read, update, delete).

151
00:09:42,350 --> 00:09:47,350
We know we need a process to retrieve rules
periodically.

152
00:09:47,350 --> 00:09:50,440
And store rules in memory.

153
00:09:50,440 --> 00:09:53,759
And we need a component that makes a decision.

154
00:09:53,759 --> 00:09:59,610
You may argue whether we need the client identifier
builder as a separate component or should

155
00:09:59,610 --> 00:10:02,420
it just be a part of the decision-making component.

156
00:10:02,420 --> 00:10:04,480
It is up to you.

157
00:10:04,480 --> 00:10:10,230
I wanted to present this builder as a separate
component to stress the point that client

158
00:10:10,230 --> 00:10:14,550
identification is an important step of the
whole process.

159
00:10:14,550 --> 00:10:18,740
From here interview may go in several directions.

160
00:10:18,740 --> 00:10:23,529
Interviewer may be interested in the Rate
Limiter algorithm and ask us to implement

161
00:10:23,529 --> 00:10:24,800
one.

162
00:10:24,800 --> 00:10:30,240
Or interviewer may be interested in object-oriented
design and ask us to define main classes and

163
00:10:30,240 --> 00:10:33,649
interfaces of the throttling library.

164
00:10:33,649 --> 00:10:40,529
Or interviewer may ask us to focus on a distributed
throttling solution and discuss how service

165
00:10:40,529 --> 00:10:42,959
hosts share data between each other.

166
00:10:42,959 --> 00:10:45,910
Let’s discuss each of these possible directions.

167
00:10:45,910 --> 00:10:47,520
And start with the algorithm.

168
00:10:47,520 --> 00:10:52,720
I will not tell you a secret if I say that
there are many different algorithms to solve

169
00:10:52,720 --> 00:10:54,180
this problem.

170
00:10:54,180 --> 00:10:59,470
You may find inspiration by looking into Google
Guava RateLimiter class.

171
00:10:59,470 --> 00:11:04,660
Or think about how fixed and sliding window
paradigms can be applied.

172
00:11:04,660 --> 00:11:08,920
But probably the simplest algorithm out there
is the Token Bucket algorithm.

173
00:11:08,920 --> 00:11:12,230
Let me describe the main idea.

174
00:11:12,230 --> 00:11:17,790
The token bucket algorithm is based on an
analogy of a bucket filled with tokens.

175
00:11:17,790 --> 00:11:24,420
Each bucket has three characteristics: a maximum
amount of tokens it can hold, amount of tokens

176
00:11:24,420 --> 00:11:31,480
currently available and a refill rate, the
rate at which tokens are added to the bucket.

177
00:11:31,480 --> 00:11:35,290
Every time request comes, we take a token
from the bucket.

178
00:11:35,290 --> 00:11:40,089
If there are no more tokens available in the
bucket, request is rejected.

179
00:11:40,089 --> 00:11:45,490
And the bucket is refilled with a constant
rate.

180
00:11:45,490 --> 00:11:50,369
The beauty of the Token Bucket algorithm is
that it simple to understand and simple to

181
00:11:50,369 --> 00:11:51,369
implement.

182
00:11:51,369 --> 00:11:52,699
Let’s take a look at the code.

183
00:11:52,699 --> 00:12:00,809
There are 4 class fields: maximum bucket size,
refill rate, number of currently available

184
00:12:00,809 --> 00:12:07,329
tokens and timestamp that indicates when bucket
was last refilled.

185
00:12:07,329 --> 00:12:12,589
Constructor accepts two arguments: maximum
bucket size and refill rate.

186
00:12:12,589 --> 00:12:17,800
Number of currently available tokens is set
to the maximum bucket size.

187
00:12:17,800 --> 00:12:23,230
And timestamp is set to the current time in
nanoseconds.

188
00:12:23,230 --> 00:12:29,080
Allow request method has one argument - number
of tokens that represent a cost of the operation.

189
00:12:29,080 --> 00:12:31,670
Usually, the cost is equal to 1.

190
00:12:31,670 --> 00:12:35,459
Meaning that with every request we take a
single token from the bucket.

191
00:12:35,459 --> 00:12:38,850
But it may be a larger value as well.

192
00:12:38,850 --> 00:12:43,910
For example, when we have a slow operation
in the web service and each request to that

193
00:12:43,910 --> 00:12:46,639
operation may cost several tokens.

194
00:12:46,639 --> 00:12:49,939
The first thing we do is refilling the bucket.

195
00:12:49,939 --> 00:12:54,459
And right after that we check if there are
enough tokens in the bucket.

196
00:12:54,459 --> 00:12:59,749
In case there are not enough tokens, method
return false, indicating that request must

197
00:12:59,749 --> 00:13:01,009
be throttled.

198
00:13:01,009 --> 00:13:07,330
Otherwise, we need to decrease number of available
tokens by the cost of the request.

199
00:13:07,330 --> 00:13:10,009
And the last piece is the refill method.

200
00:13:10,009 --> 00:13:15,499
It calculates how many tokens accumulated
since the last refill and increases currently

201
00:13:15,499 --> 00:13:18,620
available tokens in the bucket by this number.

202
00:13:18,620 --> 00:13:22,390
Let’s make sure you understand the implementation.

203
00:13:22,390 --> 00:13:27,610
Because if you do, it will be easy to implement
token bucket algorithm during a real interview.

204
00:13:27,610 --> 00:13:32,369
Let’s say in time T0 bucket was created.

205
00:13:32,369 --> 00:13:37,499
Maximum capacity is set to 10 and refill rate
is set to 10 tokens per second.

206
00:13:37,499 --> 00:13:41,839
So, the bucket currently has 10 tokens available.

207
00:13:41,839 --> 00:13:49,160
In time T1, which is 300 milliseconds later,
allow request method call was initiated and

208
00:13:49,160 --> 00:13:52,370
the cost of that request is 6 tokens.

209
00:13:52,370 --> 00:13:58,249
How many tokens have remained in the bucket
after allow request method completed?

210
00:13:58,249 --> 00:14:00,540
And the answer is 4.

211
00:14:00,540 --> 00:14:04,689
Bucket was full all this time, no new tokens
have been added to the bucket.

212
00:14:04,689 --> 00:14:08,389
So, we simply subtract 6 tokens.

213
00:14:08,389 --> 00:14:13,149
200 milliseconds later one more allow request
call was initiated.

214
00:14:13,149 --> 00:14:15,879
With the 5 tokens cost.

215
00:14:15,879 --> 00:14:19,540
How many tokens have remained after this call?

216
00:14:19,540 --> 00:14:22,750
And the answer is 1.

217
00:14:22,750 --> 00:14:27,170
First, two more tokens have been added to
the bucket by the refill method.

218
00:14:27,170 --> 00:14:30,129
And then 5 tokens have been subtracted.

219
00:14:30,129 --> 00:14:32,119
Easy, right?

220
00:14:32,119 --> 00:14:37,100
And 1 second later, actually 900 milliseconds,
bucket is full again.

221
00:14:37,100 --> 00:14:40,970
So far we have covered the algorithmic part
of the rate limiting solution.

222
00:14:40,970 --> 00:14:45,800
Let’s take a look at another facet of the
problem, which is object-oriented design.

223
00:14:45,800 --> 00:14:49,529
Let’s define key classes and interfaces.

224
00:14:49,529 --> 00:14:55,220
Job Scheduler interface is responsible for
scheduling a job that runs every several seconds

225
00:14:55,220 --> 00:14:58,319
and retrieves rules from Rules service.

226
00:14:58,319 --> 00:15:05,129
RulesCache interface is responsible for storing
rules in memory.

227
00:15:05,129 --> 00:15:10,149
ClientIdentifier builds a key that uniquely
identifies a client.

228
00:15:10,149 --> 00:15:13,819
And RateLimiter is responsible for decision
making.

229
00:15:13,819 --> 00:15:18,889
RetrieveJobScheduler class implements JobScheduler
interface.

230
00:15:18,889 --> 00:15:23,550
Its responsibility is to instantiate, start
and stop the scheduler.

231
00:15:23,550 --> 00:15:25,990
And to run retrieve rules task.

232
00:15:25,990 --> 00:15:33,689
In Java, for example, we can utilize ScheduledExecutorService
interface as a scheduler.

233
00:15:33,689 --> 00:15:37,050
TokenBucketCache stores token buckets.

234
00:15:37,050 --> 00:15:40,639
We can use something simple, for example Map
to store buckets.

235
00:15:40,639 --> 00:15:46,089
Or utilize 3-rd party cache implementation,
like Google Guava cache.

236
00:15:46,089 --> 00:15:51,730
ClientIdentifierBuilder is responsible for
building a key based on user identity information

237
00:15:51,730 --> 00:15:53,299
(for example login).

238
00:15:53,299 --> 00:15:58,939
There can be other implementations as well,
for example based on IP address.

239
00:15:58,939 --> 00:16:05,230
And for the RateLimiter interface lets introduce
a TokenBucketRateLimiter class, which is responsible

240
00:16:05,230 --> 00:16:10,660
for calling allow request on the correspondent
bucket for that client.

241
00:16:10,660 --> 00:16:15,869
And the last important piece is the RetrieveRulesTask,
which is responsible for retrieving all the

242
00:16:15,869 --> 00:16:17,790
rules for this service.

243
00:16:17,790 --> 00:16:22,420
Let’s look at how these components interact
with each other.

244
00:16:22,420 --> 00:16:26,730
Hopefully, it will help you to better remember
all the components.

245
00:16:26,730 --> 00:16:32,899
RetrieveJobScheduler runs RetrieveRulesTask,
which makes a remote call to the Rules service.

246
00:16:32,899 --> 00:16:38,179
It then creates token buckets and puts them
into the cache.

247
00:16:38,179 --> 00:16:44,600
When client request comes to the host, RateLimiter
first makes a call to the ClientIdentifierBuilder

248
00:16:44,600 --> 00:16:47,790
to build a unique identifier for the client.

249
00:16:47,790 --> 00:16:52,069
And then it passes this key to the cache and
retrieves the bucket.

250
00:16:52,069 --> 00:16:55,560
And the last step to do is to call allow request
on the bucket.

251
00:16:55,560 --> 00:17:01,829
Now, let’s step into the distributed world
and see how we can make rate limiting work

252
00:17:01,829 --> 00:17:04,369
across many machines in a cluster.

253
00:17:04,369 --> 00:17:05,749
But let me ask you something first.

254
00:17:05,750 --> 00:17:10,550
We have a cluster that consists of 3 hosts.

255
00:17:10,550 --> 00:17:16,319
And we want rate limiting solution to allow
4 requests per second for each client.

256
00:17:16,319 --> 00:17:20,299
How many tokens should we give to a bucket
on every host?

257
00:17:20,300 --> 00:17:23,260
Should we give 4 divided by 3?

258
00:17:23,260 --> 00:17:25,260
And the answer is 4.

259
00:17:25,260 --> 00:17:28,319
Each bucket should have 4 tokens initially.

260
00:17:28,319 --> 00:17:33,650
The reason for this is that all requests for
the same bucket may in theory land on the

261
00:17:33,650 --> 00:17:34,750
same host.

262
00:17:34,750 --> 00:17:40,310
Load balancers try to distributed requests
evenly, but they do not know anything about

263
00:17:40,310 --> 00:17:46,000
keys, and requests for the same key will not
be evenly distributed.

264
00:17:46,000 --> 00:17:51,340
Let's add load balancer into the picture and
run a very simple simulation.

265
00:17:51,340 --> 00:17:56,620
The first request goes to host A, one token
is consumed.

266
00:17:56,620 --> 00:18:01,440
The second request goes to host C and one
token is consumed there.

267
00:18:01,440 --> 00:18:08,860
Two other requests, within the same 1 second
interval, go to host B. And take two tokens

268
00:18:08,860 --> 00:18:10,900
from the bucket.

269
00:18:10,900 --> 00:18:16,270
All 4 allowed requests hit the cluster; we
should throttle all the remaining requests

270
00:18:16,270 --> 00:18:18,000
for this second.

271
00:18:18,000 --> 00:18:20,200
But we still have tokens available.

272
00:18:20,200 --> 00:18:21,280
What should we do?

273
00:18:21,280 --> 00:18:28,760
We must allow hosts to talk to each other
and share how many tokens they consumed altogether.

274
00:18:28,760 --> 00:18:34,120
In this case host A will see that other two
hosts consumed 3 tokens.

275
00:18:34,120 --> 00:18:36,970
And host A will subtract this number from
its bucket.

276
00:18:36,970 --> 00:18:40,410
Leaving it with 0 tokens available.

277
00:18:40,410 --> 00:18:45,530
Host B will find out that A and C consumed
two tokens already.

278
00:18:45,530 --> 00:18:48,770
Leaving host B with 0 tokens as well.

279
00:18:48,770 --> 00:18:53,200
And the same logic applies to host C. Now
everything looks correct.

280
00:18:53,200 --> 00:18:56,570
4 requests have been processed and no more
requests allowed.

281
00:18:56,570 --> 00:18:58,800
I bet you have a question.

282
00:18:58,800 --> 00:19:02,310
We gave each bucket 4 tokens.

283
00:19:02,310 --> 00:19:08,020
If many requests for the same bucket hit our
cluster exactly at the same second.

284
00:19:08,020 --> 00:19:11,880
Does this mean that 12 requests may be processed,
instead of only 4 allowed?

285
00:19:11,880 --> 00:19:15,330
Or may be a more realistic scenario.

286
00:19:15,330 --> 00:19:20,860
Because communication between hosts takes
time, until all hosts agree on what that final

287
00:19:20,860 --> 00:19:27,560
number of tokens must be, may there be any
requests that slip into the system at that

288
00:19:27,560 --> 00:19:28,560
time?

289
00:19:28,560 --> 00:19:29,560
Yes.

290
00:19:29,560 --> 00:19:31,210
Unfortunately, this is the case.

291
00:19:31,210 --> 00:19:36,660
We should expect that sometimes our system
may be processing more requests than we expect

292
00:19:36,660 --> 00:19:39,040
and we need to scale out our cluster accordingly.

293
00:19:39,040 --> 00:19:43,900
By the way, the token bucket algorithm will
still handle this use case well.

294
00:19:43,900 --> 00:19:48,940
We just need to slightly modify it to allow
negative number of available tokens.

295
00:19:48,940 --> 00:19:54,840
When 12 requests hit the system, buckets will
start sharing this information.

296
00:19:54,840 --> 00:20:01,440
After sharing, every bucket will have -8 tokens
and for the duration of the next 2 seconds

297
00:20:01,440 --> 00:20:03,290
all requests will be throttled.

298
00:20:03,290 --> 00:20:09,220
So, on average we processed 12 requests within
3 seconds.

299
00:20:09,220 --> 00:20:13,350
Although in reality all 12 were processed
within the first second.

300
00:20:13,350 --> 00:20:16,460
So, communication between hosts is the key.

301
00:20:16,460 --> 00:20:19,700
Let’s see how this communication can be
implemented.

302
00:20:19,700 --> 00:20:25,930
By the way, ideas we will discuss next are
applicable not only for rate limiting solution,

303
00:20:25,930 --> 00:20:30,870
but many other distributed systems that require
data sharing between all hosts in a cluster

304
00:20:30,870 --> 00:20:32,610
in a real time.

305
00:20:32,610 --> 00:20:36,060
The first approach is to tell everyone everything.

306
00:20:36,060 --> 00:20:41,900
It means that every host in the cluster knows
about every other host in the cluster and

307
00:20:41,900 --> 00:20:44,750
share messages with each one of them.

308
00:20:44,750 --> 00:20:49,550
You may also heard a term full mesh that describes
this network topology.

309
00:20:49,550 --> 00:20:53,150
How do hosts discover each other?

310
00:20:53,150 --> 00:20:57,120
When a new host is added, how does everyone
else know?

311
00:20:57,120 --> 00:21:01,190
And there are several approaches used for
hosts discovery.

312
00:21:01,190 --> 00:21:06,740
One option is to use a 3-rd party service
which will listen to heartbeats coming from

313
00:21:06,740 --> 00:21:08,230
every host.

314
00:21:08,230 --> 00:21:13,060
As long as heartbeats come, host is keep registered
in the system.

315
00:21:13,060 --> 00:21:19,110
If heartbeats stop coming, the service unregister
host that is no longer alive.

316
00:21:19,110 --> 00:21:25,610
And all hosts in our cluster ask this 3-rd
party service for the full list of members.

317
00:21:25,610 --> 00:21:28,930
Another option is to resolve some user provided
information.

318
00:21:28,930 --> 00:21:34,780
For example, user specifies a VIP and because
VIP knows about all the hosts behind it, we

319
00:21:34,780 --> 00:21:38,750
can use this information to obtain all the
members.

320
00:21:38,750 --> 00:21:43,920
Or we can rely on a less flexible but still
a good option when user provides a list of

321
00:21:43,920 --> 00:21:46,650
hosts via some configuration file.

322
00:21:46,650 --> 00:21:54,390
We then need a way to deploy this file across
all cluster nodes every time this list changes.

323
00:21:54,390 --> 00:21:58,830
Full mesh broadcasting is relatively straightforward
to implement.

324
00:21:58,830 --> 00:22:03,080
But the main problem with this approach is
that it is not scalable.

325
00:22:03,080 --> 00:22:09,530
Number of messages grows quadratically with
respect to the number of hosts in a cluster.

326
00:22:09,530 --> 00:22:14,460
Approach works well for small clusters, but
we will not be able to support big clusters.

327
00:22:14,460 --> 00:22:19,790
So, let’s investigate some other options
that may require less messages to be broadcasted

328
00:22:19,790 --> 00:22:21,060
within the cluster.

329
00:22:21,060 --> 00:22:24,060
And one such option is to use a gossip protocol.

330
00:22:24,060 --> 00:22:28,390
This protocol is based on the way that epidemics
spread.

331
00:22:28,390 --> 00:22:33,580
Computer systems typically implement this
type of protocol with a form of random "peer

332
00:22:33,580 --> 00:22:39,820
selection": with a given frequency, each machine
picks another machine at random and shares

333
00:22:39,820 --> 00:22:40,820
data.

334
00:22:40,820 --> 00:22:44,810
By the way, rate limiting solution at Yahoo
uses this approach.

335
00:22:44,810 --> 00:22:48,080
Next option is to use distributed cache cluster.

336
00:22:48,080 --> 00:22:49,780
For example, Redis.

337
00:22:49,780 --> 00:22:52,890
Or we can implement custom distributed cache
solution.

338
00:22:52,890 --> 00:22:58,350
The pros for this approach is that distributed
cache cluster is relatively small and our

339
00:22:58,350 --> 00:23:01,450
service cluster can scale out independently.

340
00:23:01,450 --> 00:23:06,570
This cluster can be shared among many different
service teams in the organization.

341
00:23:06,570 --> 00:23:10,440
Or each team can setup their own small cluster.

342
00:23:10,440 --> 00:23:13,410
Next approach also relies on a 3-rd party
component.

343
00:23:13,410 --> 00:23:16,290
A coordination service that helps to choose
a leader.

344
00:23:16,290 --> 00:23:22,040
Choosing a leader helps to decrease number
of messages broadcasted within the cluster.

345
00:23:22,040 --> 00:23:27,360
Leader asks everyone to send it all the information.

346
00:23:27,360 --> 00:23:30,950
And then it calculates and sends back the
final result.

347
00:23:30,950 --> 00:23:35,890
So, each host only needs to talk to a leader
or a set of leaders, where each leader is

348
00:23:35,890 --> 00:23:39,030
responsible for its own range of keys.

349
00:23:39,030 --> 00:23:43,900
Consensus algorithms such as Paxos and Raft
can be used to implement Coordination Service.

350
00:23:43,900 --> 00:23:49,530
Great option, but the main drawback is that
we need to setup and maintain Coordination

351
00:23:49,530 --> 00:23:51,860
Service.

352
00:23:51,860 --> 00:23:56,420
Coordination service is typically a very sophisticated
component that has to be very reliable and

353
00:23:56,420 --> 00:24:00,070
make sure one and only one leader is elected.

354
00:24:00,070 --> 00:24:02,610
But is this really a requirement for our system?

355
00:24:02,610 --> 00:24:07,090
Let’s say we use a simple algorithm to elect
a leader.

356
00:24:07,090 --> 00:24:13,380
But because of the simplicity of the algorithm
it may not guarantee one and only one leader.

357
00:24:13,380 --> 00:24:16,700
So that we may end up with multiple leaders
being elected.

358
00:24:16,700 --> 00:24:18,520
Is this an issue?

359
00:24:18,520 --> 00:24:19,620
Actually, no.

360
00:24:19,620 --> 00:24:23,770
Each leader will calculate rate and share
with everyone else.

361
00:24:23,770 --> 00:24:28,370
This will cause unnecessary messaging overhead,
but each leader will have its own correct

362
00:24:28,370 --> 00:24:31,450
view of the overall rate.

363
00:24:31,450 --> 00:24:36,030
And to finish message broadcasting discussion,
I want to talk about communication protocols,

364
00:24:36,030 --> 00:24:39,030
how hosts talk to each other.

365
00:24:39,030 --> 00:24:42,770
We have two options here: TCP and UDP.

366
00:24:42,770 --> 00:24:48,590
TCP protocol guarantees delivery of data and
also guarantees that packets will be delivered

367
00:24:48,590 --> 00:24:51,900
in the same order in which they were sent.

368
00:24:51,900 --> 00:24:58,570
UDP protocol does not guarantee you are getting
all the packets and order is not guaranteed.

369
00:24:58,570 --> 00:25:03,350
But because UDP throws all the error-checking
stuff out, it is faster.

370
00:25:03,350 --> 00:25:06,060
So, which one is better?

371
00:25:06,060 --> 00:25:07,060
Both are good choices.

372
00:25:07,060 --> 00:25:13,060
If we want rate limiting solution to be more
accurate, but with a little bit of performance

373
00:25:13,060 --> 00:25:15,130
overhead, we need to go with TCP.

374
00:25:15,130 --> 00:25:21,090
If we ok to have a bit less accurate solution,
but the one that works faster, UDP should

375
00:25:21,090 --> 00:25:22,420
be our choice.

376
00:25:22,420 --> 00:25:28,880
Ok, we have implemented the algorithm, created
a set of classes and interfaces, discussed

377
00:25:28,880 --> 00:25:30,910
message broadcasting.

378
00:25:30,910 --> 00:25:35,181
But how do we integrate all this cool solution
with the service?

379
00:25:35,181 --> 00:25:36,720
Let’s see what options we have.

380
00:25:36,720 --> 00:25:38,020
There are two options.

381
00:25:38,020 --> 00:25:39,820
And they are pretty standard.

382
00:25:39,820 --> 00:25:46,760
We can run Rate Limiter as a part of the service
process or as its own process (daemon).

383
00:25:46,760 --> 00:25:52,150
In the first option, Rate Limiter is distributed
as a collection of classes, a library that

384
00:25:52,150 --> 00:25:55,140
should be integrated with the service code.

385
00:25:55,140 --> 00:26:00,320
In the second option we have two libraries:
the daemon itself and the client, that is

386
00:26:00,320 --> 00:26:06,120
responsible for inter-process communication
between the service process and the daemon.

387
00:26:06,120 --> 00:26:09,160
Client is integrated with the service code.

388
00:26:09,160 --> 00:26:11,050
What are the pros for the first approach?

389
00:26:11,050 --> 00:26:15,750
It is faster, as we do not need to do any
inter-process call.

390
00:26:15,750 --> 00:26:21,850
It is also resilient to the inter-process
call failures, because there are no such calls.

391
00:26:21,850 --> 00:26:25,100
The second approach is programming language
agnostic.

392
00:26:25,100 --> 00:26:30,830
It means that Rate Limiter daemon can be written
on a programming language that may be different

393
00:26:30,830 --> 00:26:33,760
from the language we use for the service implementation.

394
00:26:33,760 --> 00:26:36,850
As we do not need to do integration on the
code level.

395
00:26:36,850 --> 00:26:41,710
Yes, we need to have Rate Limiter client compatible
with the service code language.

396
00:26:41,710 --> 00:26:43,810
But not the daemon itself.

397
00:26:43,810 --> 00:26:49,270
Also, Rate Limiter process uses its own memory
space.

398
00:26:49,270 --> 00:26:54,750
This isolation helps to better control behavior
for both the service and the daemon.

399
00:26:54,750 --> 00:26:59,610
For example, daemon my store many buckets
in memory, but because the service process

400
00:26:59,610 --> 00:27:04,900
has its own memory space, the service memory
does not need to allocate space for these

401
00:27:04,900 --> 00:27:05,900
buckets.

402
00:27:05,900 --> 00:27:09,670
Which makes service memory allocation more
predictable.

403
00:27:09,670 --> 00:27:14,570
Another good reason, and you may see it happening
a lot in practice, service teams tend to be

404
00:27:14,570 --> 00:27:19,890
very cautious when you come to them and ask
to integrate their service with your super

405
00:27:19,890 --> 00:27:21,980
cool library.

406
00:27:21,980 --> 00:27:24,210
You will hear tons of questions.

407
00:27:24,210 --> 00:27:27,710
Like how much memory and CPU your library
consumes?

408
00:27:27,710 --> 00:27:32,400
What will happen in case of a network partition
or any other exceptional scenario?

409
00:27:32,400 --> 00:27:35,420
Can we see results of the load testing for
your library?

410
00:27:35,420 --> 00:27:37,640
What are your mom’s favorite flowers?

411
00:27:37,640 --> 00:27:39,480
And many many other questions.

412
00:27:39,480 --> 00:27:42,800
These questions are also applicable to the
daemon solution.

413
00:27:42,800 --> 00:27:48,740
But it is easier to guarantee that the service
itself will not be impacted by any bugs that

414
00:27:48,740 --> 00:27:51,630
may be in the Rate Limiter library.

415
00:27:51,630 --> 00:27:57,320
As you may see, strengths of the first approach
become weaknesses of the second approach.

416
00:27:57,320 --> 00:27:58,580
And vice versa.

417
00:27:58,580 --> 00:28:01,360
So, which option is better?

418
00:28:01,360 --> 00:28:05,710
Both are good options and it really depends
on the use cases and needs of a particular

419
00:28:05,710 --> 00:28:06,790
service team.

420
00:28:06,790 --> 00:28:12,080
By the way, the second approach, when we have
a daemon that communicates with other hosts

421
00:28:12,080 --> 00:28:16,340
in the cluster is a quite popular pattern
in distributed systems.

422
00:28:16,340 --> 00:28:21,260
For example, it is widely used to implement
auto discovery of service hosts, when hosts

423
00:28:21,260 --> 00:28:23,190
in a cluster identify each other.

424
00:28:23,190 --> 00:28:27,970
Now let’s see what else an interviewer may
want to discuss with us.

425
00:28:27,970 --> 00:28:34,480
In theory, it is possible that many token
buckets will be created and stored in memory.

426
00:28:34,480 --> 00:28:38,840
For example, when millions of clients send
requests at the same second.

427
00:28:38,840 --> 00:28:44,650
In practice though, we do not need to keep
buckets in memory if there are no requests

428
00:28:44,650 --> 00:28:48,280
coming from the client for some period of
time.

429
00:28:48,280 --> 00:28:53,990
For example, client made its first request
and we created a bucket.

430
00:28:53,990 --> 00:28:59,070
As long as this client continues to send requests
and interval between these requests is less

431
00:28:59,070 --> 00:29:03,530
than a second or couple of seconds, we keep
the bucket in memory.

432
00:29:03,530 --> 00:29:08,640
If there are no requests coming for this bucket
for several seconds, we can remove the bucket

433
00:29:08,640 --> 00:29:10,030
from memory.

434
00:29:10,030 --> 00:29:15,270
And bucket will be re-created again when client
makes a new request.

435
00:29:15,270 --> 00:29:18,740
As for failure modes, there may be several
of them.

436
00:29:18,740 --> 00:29:24,380
Daemon can fail, causing other hosts in the
cluster lose visibility of this failed daemon.

437
00:29:24,380 --> 00:29:30,020
In the result, the host with a failed daemon
leaves the group and continues to throttle

438
00:29:30,020 --> 00:29:33,180
requests without talking to other hosts in
the cluster.

439
00:29:33,180 --> 00:29:36,030
Nothing really bad happens.

440
00:29:36,030 --> 00:29:39,730
Just less requests will be throttled in total.

441
00:29:39,730 --> 00:29:44,720
And we will have similar results in case of
a network partition, when several hosts in

442
00:29:44,720 --> 00:29:49,920
the cluster may not be able to broadcast messages
to the rest of the group.

443
00:29:49,920 --> 00:29:52,990
Just less requests throttled in total.

444
00:29:52,990 --> 00:29:58,510
And if you wonder why, just remember our previous
example with 3 hosts and 4 tokens.

445
00:29:58,510 --> 00:30:02,950
If hosts talk to each other, only 4 requests
are allowed across all of them.

446
00:30:02,950 --> 00:30:08,670
If hosts do not talk to each other due to
let’s say network issues, each host will

447
00:30:08,670 --> 00:30:11,160
allow 4 requests, 12 in total.

448
00:30:11,160 --> 00:30:16,680
So, in case of failures in our rate limiter
solution, more requests are allowed and less

449
00:30:16,680 --> 00:30:18,150
requests are throttled.

450
00:30:18,150 --> 00:30:23,720
With regards to rule management, we may need
to introduce a self-service tool, so that

451
00:30:23,720 --> 00:30:28,760
service teams may create, update and delete
their rules when needed.

452
00:30:28,760 --> 00:30:34,340
As for synchronization, there may be several
places where we need it.

453
00:30:34,340 --> 00:30:37,970
First, we have synchronization in the token
bucket.

454
00:30:37,970 --> 00:30:45,470
There is a better way to implement thread-safety
in that class, using for example atomic references.

455
00:30:45,470 --> 00:30:49,230
Another place that may require synchronization
is the token bucket cache.

456
00:30:49,230 --> 00:30:54,830
As we mentioned before, if there are too many
buckets stored in the cache and we want to

457
00:30:54,830 --> 00:31:00,880
delete unused buckets and re-create them when
needed, we will end up with synchronization.

458
00:31:00,880 --> 00:31:06,380
So, we may need to use concurrent hash map,
which is a thread safe equivalent of the hash

459
00:31:06,380 --> 00:31:07,710
map in Java.

460
00:31:07,710 --> 00:31:13,030
In general, no need to be afraid of the synchronization
in both those places.

461
00:31:13,030 --> 00:31:18,070
It may become a bottleneck eventually, but
only for services with insanely large requests

462
00:31:18,070 --> 00:31:19,450
per second rate.

463
00:31:19,450 --> 00:31:24,100
For most services out there even the simplest
synchronization implementation does not add

464
00:31:24,100 --> 00:31:26,030
to much overhead.

465
00:31:26,030 --> 00:31:29,890
Hopefully, this is the last question from
the interviewer.

466
00:31:29,890 --> 00:31:35,920
So, what clients of our service should do
with throttled calls?

467
00:31:35,920 --> 00:31:38,720
There are several options, as always.

468
00:31:38,720 --> 00:31:41,790
Clients may queue such requests and re-send
them later.

469
00:31:41,790 --> 00:31:45,150
Or they can retry throttled requests.

470
00:31:45,150 --> 00:31:51,000
But do it in a smart way, and this smart way
is called exponential backoff and jitter.

471
00:31:51,000 --> 00:31:52,490
Probably too smart.

472
00:31:52,490 --> 00:31:55,420
But do not worry, ideas are quite simple.

473
00:31:55,420 --> 00:32:00,820
An exponential backoff algorithm retries requests
exponentially, increasing the waiting time

474
00:32:00,820 --> 00:32:04,130
between retries up to a maximum backoff time.

475
00:32:04,130 --> 00:32:10,860
In other words, we retry requests several
times, but wait a bit longer with every retry

476
00:32:10,860 --> 00:32:12,000
attempt.

477
00:32:12,000 --> 00:32:16,920
And jitter adds randomness to retry intervals
to spread out the load.

478
00:32:16,920 --> 00:32:22,360
If we do not add jitter, backoff algorithm
will retry requests at the same time.

479
00:32:22,360 --> 00:32:24,550
And jitter helps to separate retries.

480
00:32:24,550 --> 00:32:29,450
Now let’s summarize what we have discussed
so far.

481
00:32:29,450 --> 00:32:33,710
Service owners can use a self-service tools
for rules management.

482
00:32:33,710 --> 00:32:35,900
Rules are stored in the database.

483
00:32:35,900 --> 00:32:43,420
On the service host we have rules retriever
that stores retrieved rules in the local cache.

484
00:32:43,420 --> 00:32:49,460
When request comes, rate limiter client builds
client identifier and passes it to the rate

485
00:32:49,460 --> 00:32:51,760
limiter to make a decision.

486
00:32:51,760 --> 00:32:57,160
Rate limiter communicates with a message broadcaster,
that talks to other hosts in the cluster.

487
00:32:57,160 --> 00:33:00,309
Let’s recall non-functional requirements.

488
00:33:00,309 --> 00:33:06,290
We wanted to build a solution that is highly
scalable, fast and accurate.

489
00:33:06,290 --> 00:33:11,100
And at this point I would really like to say
that the solution we have built meets all

490
00:33:11,100 --> 00:33:12,510
the requirements.

491
00:33:12,510 --> 00:33:13,870
But this is not completely true.

492
00:33:13,870 --> 00:33:17,700
And the correct answer is “it depends”.

493
00:33:17,700 --> 00:33:22,190
Depends on the number of hosts in the cluster,
depends on the number of rules, depends on

494
00:33:22,190 --> 00:33:23,950
the request rate.

495
00:33:23,950 --> 00:33:29,150
For majority of clusters out there, where
cluster size is less then several thousands

496
00:33:29,150 --> 00:33:36,520
of nodes and number of active buckets per
second is less then tens of thousands, gossip

497
00:33:36,520 --> 00:33:40,410
communication over UDP will work really fast
and is quite accurate.

498
00:33:40,410 --> 00:33:46,210
In case of a rally large clusters, like tens
of thousands of hosts, we may no longer rely

499
00:33:46,210 --> 00:33:52,210
on host-to-host communication in the service
cluster as it becomes costly.

500
00:33:52,210 --> 00:33:55,830
And we need a separate cluster for making
a throttling decision.

501
00:33:55,830 --> 00:33:59,070
This is a distributed cache option we discussed
above.

502
00:33:59,070 --> 00:34:03,270
But the drawback of this approach is that
it increases latency and operational cost.

503
00:34:03,270 --> 00:34:07,060
It would be good to have these tradeoff discussions
with your interviewer.

504
00:34:07,060 --> 00:34:12,000
As it demonstrates both breadth and depth
of your knowledge and critical thinking.

505
00:34:12,000 --> 00:34:16,040
But do not worry if you are not at the point
yet where you feel comfortable discussing

506
00:34:16,040 --> 00:34:17,830
all these details.

507
00:34:17,830 --> 00:34:22,168
Keep watching video on this channel and I
really hope that together we will be able

508
00:34:22,168 --> 00:34:24,599
to improve your system design skills.

509
00:34:24,600 --> 00:34:26,889
That is it for today’s interview question.

510
00:34:26,889 --> 00:34:30,579
As always, if you have any questions please
leave them in the comments below.

511
00:34:30,580 --> 00:34:33,639
In the next video we will design a distributed
cache.

512
00:34:33,639 --> 00:34:35,559
So, I will see you soon.

