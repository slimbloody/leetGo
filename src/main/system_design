系统设计中常说的Tradeoff是什么
什么叫做SOA (Service Oriented Architecture)
什么是Pull Model &什么是Push Model
数据存储系统有哪些,什么样的数据适合存在什么样的数据存储系统中
什么是异步任务和消息队列(Message Queue)
什么是数据的可持久化(Persistent)
什么是去标准化(Denormalize)
什么是惊群效应(Thundering Herd)
有哪些与NewsFeed类似的系统设计问题?


●设计某某系统Design XXX System
    ●设计微博Design Twitter
    ●设计人人Design Facebook
    ●设计滴滴Design Uber
    ●设计微信Design Whatsapp
    ●设计点评Design Yelp
    ●设计短网址系统Design Tiny URL
    ●设计NoSQL数据库Design NoSQL

●设计某某系统中的某某功能
    ●设计一个功能实现对用户访问频率的限制(输错密码的频率5次限制)
    ●设计一个功能实现统计某个具体事件的历史发生次数(链接在每年被点了多少次)
    ●设计删除一个Tweet的功能
    ●设计邮件系统中将所有邮件标记为已读的功能

系统设计很多时候要做到o(1), 要从数据库的角度上讲怎么实现

设计邮件系统中将所有邮件标记为已读的功能
加一个时间戳记录, xxx时间点过这个按钮
如果时间在xxx之后, 并且读的flag为false, 才是没读
优化掉了写的时间

把上面标记过全部已读的邮件, 从里面再标记为未读
再加一个update_time




1.Work solution
2.Analysis and communication
对设计出什么样的feature达成一致
对存储空间和带宽进行分析
3.Tradeoff Pros/Cons
4.Knowledge Base


步骤
Step 1: Clarify the requirements
Step 2: Capacity Estimation
数据库系统带宽规模估计
Step 3: System APIs
Step 4: High-level System Design
Step 5: Data Storage
Step 6: Scalability

一到两个重点feature考察系统设计的基本功









======================
twitter:
======================
1.Tweet
    a.Create
    b.Delete
2.Timeline/Feed
    а.Home
    b.User
3.Follow a user
4.Like a tweet
5.Search tweets



Consistency
    Every read receives the most recent write or an error
C
    Sacrifice: Eventual consistency(不会很影响用户体验)
Availability
    Every request receives a (non-error) response, without the guarantee that it contains the
most recent write
    Scalable
        Performance: low Latency

Partition tolerance (Fault Tolerance):
1.系统丢包了
2.系统server down了
3.磁盘坏了
The system continues to operate despite an arbitrary number of messages being
dropped (or delayed) by the network between nodes


Step 2: Capacity Estimation
. Assumption:
200 million DAU, 100 million new tweets
Each user: visit home timeline 5 times; other user timeline 3 times
Each timeline/page has 20 tweets
Each tweet has size 280 (140 characters) bytes,
metadata(1.发布时间 2.location) 30 bytes
    Per photo: 200KB, 20% tweets have images
    Per video: 2MB, 10% tweets have video, 30% videos will be watched



Storage Estimate
Write size daily:
1.Text
100M new tweets* (280 + 30) Bytes/tweet = 31GB/day
2.Image
100M new tweets * 20% has image * 200 KB per image = 4TB/day
3.Video
100M new tweets * 10% has video * 2MB per video = 20TB/day

Total
31GB + 4TB + 20TB = 24TB/day



一般都是read heavy的
Bandwidth Estimate
Daily Read Tweets Volume:
200M * (5 home visit +3 user visit) * 20 tweets/page = 32B tweets/day
Daily Read Bandwidth
Text: 32B * 280 bytes 186400 = 100MB/s
Image: 32B * 20% tweets has image * 200 KB per image /86400 = 14GB/s
Video: 32B * 10% tweets has video * 30% got watched * 2MB per video 1 86400 = 20GB/s
Total: 35GB/s

知道哪里是bottle neck, 如何scale这个系统

Step 3: System APls
postTweet(userToken, string tweet)
deleteTweet(userToken, string tweetld)
likeOrUnlikeTweet(userToken, string tweetId, bool like)
readHomeTimeLine(userToken, int pageSize, opt string pageToken)
readUserTimeline(userToken, int pageSize, opt string pageToken)
pageSize: 每次返回多少条
pageToken: 当前读到page的位置, 如果不提供的话会提供最新的tweets

Step 4: High-level System Design
user -> load balancer -> tweet writer -> db
                                |------>写入cache


fan out on write: 扩散写
写扩散也叫：Push、Fan-out或者Write-fanout
读扩散也叫：Pull、Fan-in或者Read-fanout

Home Timeline (cont'd)
Naive solution: Pull mode
How:Fetch tweets from N followers from DB, merge and return
Pros:Write is fast: O(1)
Cons:Read is slow: O(N) DB reads


Home Timeline (cont'd)
Better solution: Push mode
How
1.Maintain a feed list in cache for each user
2.Fanout on write(给每个订阅的用户更新)
Pros
1.Read is fast: O(1) from the feed list in cache
Cons
1.Write needs more efforts: O(N) write for each new tweet:Async tasks
2.Delay in showing latest tweets (eventual consistency)
latency 对用户来说没那么敏感


Fan out on write
Not efficient for users
with huge amount of followers (~>10k)

Hybrid Solution
Non-hot users:
fan out on write (push): write to user timeline cache
do not fanout on non-active users

Hot users:
fan in on read (pull): read during timeline request from tweets cache, and aggregate with
results from non-hot users
大v在读的时候和cache merge在一起不需要更新到用户的timeline里面了


Data Storage
SQL database:E.g: user table
NoSQL database:E.g: timelines
File system:Media file: image, audio, video

Step 6: Scalability
Identify potential bottlenecks
Discussion solutions, focusing on tradeoffs
Data sharding:Data store, cache
data分布在不同的区域, 让traffic read 更scalable

Load balancing:
E.g:
user <-> application server
application server <-> cache server
application server <-> db
大量请求如何assign到server上使每个server更balance
Data caching:Read heavy(大大降低read latency)





sharding: 解决 horizontal scaling的问题
why:Impossible to store/process all data in a single machine
How:Break large tables into smaller shards on multiple servers
Pros:Horizontal scaling
Cons:Complexity (distributed query, resharding)

硬盘和机器增长的速度大于新的tweets产生的速度, 系统就可以无限扩展下去
增加了额外的复杂性

distribute query:
有networking等各种问题
1.比如一个shard比较慢或者宕机了怎么办
2.数据快存满了有resharding的问题

表划分的design option
Option 1: Shard by tweet's creation time:按日期划分
Pros: Limited shards to query
Cons:
1.Hot/Cold data issue
2.New shards fill up quickly


Option 2: Shard by hash(userld): store all the data of a user on a single shard
Pros:
1.Simple
2.Query user timeline is straightforward
Cons:
1.Home timeline still needs to query multiple shards
2.Non-uniform distribution of storage:User data might not be able to fit into a single shard
3.Hot users
4.Availability


Option 3: Shard by hash(tweetld)
Pros:
1.Uniform distribution
2.High availability
Cons:
1.Need to query all shards in order to generate user 1 home timeline


social network: heavy read traffic
read 比 write 高几个数量级

存在cache里面避免hit db
Why:
Social networks have heavy read traffic
(Distributed) queries can be slow and costly

How:
Store hot / precomputed data in memory, reads can be much faster

Timeline service
User timeline: user_id -> {tweet_id}, 缓存个几千条就行了
# Varies alot, T = 1k~100k, Trump: ~60k 缓存个几千条就行了
Home timeline: user_ id -> {tweet_id} # This can be huge, sum(followee' tweets)
Tweets: tweet_ id -> tweet
# Common data that can be shared

feed流放几千条就行了
twitter 只能滚动翻页, 实际上不支持翻页, 前端设计降低后端压力

Topics:
Caching policy: LRU LFU
Sharding: 不可能一个cache server 存下来
Performance: cache 开多少内存
这里最好讲一下你把什么东西存到了cache里，大概估算的用到多少cache的capacity之类的，不然的话可能听者会觉得这么多东西都放到cache里，内存够用么

db: source of truth

=================
Autocomplete
=================
What is system design?
Systems design is the process of defining the architecture, modules, interfaces, and data for a system to
satisfy specified requirements. (wikipedia)

Clarify the requirement
1.SAL (qps 1 latency)
2.Where are the data come from
3.Amount of the data, memory 1 disk 1 cpu 1 machines

Design
1.Architecture / algorithm / data structure 1 DB schema
2.How to scale up? shard VS replica
动态产生replica
3.Backend 1 frontend 1 interface
4.Justify your decision, what are the trade offs?

补全都是热门的补全
每type一个字符都发了一个请求, 返回了前10个最佳的hit
相同的type在前端会保存一个小时
cache-control: private, max-age: 3600
expire: Tue, 19 Feb 2019 01:26:27 GMT


Question: Design a autocomplete system for YouTube
Q: How many entries?
A: 10 Billions.

Q: Where is data coming from?
A: logs.

Q: What are the latency requirements?
A: 100 ms 99 percentile.

Q: VM spec?
A: 4 CPUS, 16GB RAM, 20GB shared disk.




